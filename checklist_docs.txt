Understood. I’ll begin creating a fully detailed and structured version of the original checklist, rewritten to provide implementation-ready, context-specific instructions directly referencing code structure within a single monolithic script. All entries will be doubled in specificity, include exact locations (e.g., method names, line references), and progressively build toward a full, coherent single-file system.

I’ll let you know as soon as the updated markdown is ready for review.


# Single-File PyTorch Active Inference Agent: Koopman NCA, Wavelet Latents, Complex Memory, JEPA & More

## 1. Initialize Single-File Environment and Dependencies

Begin by **setting up the Python script with all necessary imports and configurations**. At the very top of the file, import PyTorch (`torch` and `torch.nn as nn`), PyBullet for simulation (`pybullet as p`), OpenCV for visualization (`cv2`), and any GUI library for ImGui (for example, `import imgui` or `from dearpygui import core, simple` if using DearPyGui). Also import math libraries as needed. Immediately set the **device (CPU/GPU)** and ensure reproducibility by fixing random seeds. For example:

```python
import torch, torch.nn as nn
import torch.nn.functional as F
import pybullet as p
import cv2
# If using pyimgui for ImGui interface:
import imgui
import imgui.integrations.opengl as gl_imgui
# Additional imports (e.g., math, numpy) as needed
import math, numpy as np

# Set device for PyTorch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.manual_seed(0)
```

Place the **PyBullet simulation setup** next. Connect to the physics server in DIRECT mode for headless simulation or GUI mode for visual debug. For example, `p.connect(p.DIRECT)` or `p.connect(p.GUI)` at the start of the script. Load your environment (e.g., plane, robot URDFs) and set gravity. This initialization code lives at the global scope of the script, so it runs on import. For instance:

```python
p.connect(p.DIRECT)  # Use GUI for interactive mode
p.setGravity(0, 0, -9.81)
plane_id = p.loadURDF("plane.urdf")
robot_id = p.loadURDF("robot.urdf", basePosition=[0,0,0.1])
```

**Configure simulation parameters** such as time step (`p.setTimeStep(dt)`) and possibly disable realistic timing for faster-than-real-time training (`p.setRealTimeSimulation(0)`). Also initialize any data structures to hold observations (e.g., lists for state trajectories if needed). At this stage, define how to extract the environment state for the agent: for example, get the robot’s joint angles or camera image. For a camera observation, use `p.getCameraImage()` to obtain an RGB array and depth, then convert to a PyTorch tensor for the model. If using state vectors (e.g., joint positions), fetch them with PyBullet API (e.g., `p.getJointStates`) and assemble a PyTorch tensor. **This code runs in the main loop, but declare the methods now** for clarity. For instance, you might write a helper function in the script:

```python
def get_observation():
    # Example: get a camera image and convert to torch tensor
    width, height, rgba_img, depth_img, seg_img = p.getCameraImage(128, 128)
    rgb_array = np.array(rgba_img, dtype=np.uint8)[:,:,:3]  # strip alpha
    rgb_tensor = torch.from_numpy(rgb_array).float().permute(2,0,1) / 255.0
    return rgb_tensor.to(device)
```

Ensure that **all data moves to the PyTorch device** (`.to(device)`) so that subsequent neural network computations happen on GPU if available. By setting up the environment retrieval at the top-level, we clearly mark the source of real data that feeds into the agent’s model, forming the basis for active inference loops.

## 2. Define Koopman-Enhanced Neural Cellular Automata (NCA) Module

Create a PyTorch module for the **Neural Cellular Automata (NCA)** dynamics, integrating a **Koopman operator approach** for global linear updates. This class will encapsulate the agent’s world-model dynamics at the latent level. Use `nn.Module` as base and define layers in `__init__`, then specify the update rules in `forward()`. For example:

```python
class NCADynamics(nn.Module):
    def __init__(self, channels=16, grid_size=(16,16), koopman_dim=8):
        super(NCADynamics, self).__init__()
        self.channels = channels
        self.grid_size = grid_size
        # Define local update convolution (e.g., 3x3 conv for CA neighborhood)
        self.update_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        # Define a linear layer to act as Koopman operator in a reduced space
        self.koopman_linear = nn.Linear(koopman_dim, koopman_dim, bias=False)
        # Optionally, a projection to and from Koopman subspace
        self.encoder = nn.Linear(channels, koopman_dim, bias=False)
        self.decoder = nn.Linear(koopman_dim, channels, bias=False)
    def forward(self, x):
        # x is expected to be a tensor of shape [batch, channels, H, W]
        # 1. Local NCA update
        dx_local = self.update_conv(x)               # local rule application
        # 2. Koopman global update:
        # Flatten spatially and project to Koopman subspace
        B, C, H, W = x.shape
        x_flat = x.view(B, C, -1).mean(dim=-1)       # global average pooling (C features per batch)
        z = self.encoder(x_flat)                    # project to Koopman coordinates
        z_next = self.koopman_linear(z)             # linear evolution in Koopman space
        x_global_update = self.decoder(z_next)      # back to original feature dimension
        # Expand global update to spatial grid by broadcasting
        dx_global = x_global_update.view(B, C, 1, 1).expand(-1, -1, H, W)
        # 3. Combine local and global updates
        dx = dx_local + dx_global
        # 4. Apply update (e.g., Euler integration step)
        x_next = x + dx
        # Activation (e.g., cellular automata typically clamp or nonlinear activation)
        x_next = torch.tanh(x_next)  # ensure states remain bounded
        return x_next
```

In this code:

* **Local update**: We use a convolution `update_conv` to implement the NCA’s local rule. This convolution operates on the latent grid (e.g., a 2D feature map representing the agent’s internal world state) and produces a change `dx_local`. The convolution kernels act like learned cellular automaton rules looking at a neighborhood (3x3 here).
* **Koopman global update**: We integrate a linear **Koopman operator** on a reduced state. We first encode the current latent state `x` into a lower-dimensional representation `z` (of size `koopman_dim`) using `self.encoder`. Here, we took a simple approach of **global average pooling** (averaging spatially) and then a linear projection to obtain `z` – effectively extracting a coarse global state. The `koopman_linear` (an `nn.Linear` without bias) acts as the Koopman operator, advancing `z` linearly: `z_next = K * z`. This models large-scale dynamics in a linear subspace, inspired by approaches that linearize nonlinear dynamics to identify emergent patterns. We then project back with `decoder` to get a global update in the original latent space dimensions. The result `dx_global` is broadcast across the spatial grid (making the reasonable assumption that the global Koopman-mode update is spatially homogeneous). This **captures global, emergent dynamics** that the local CNN update alone might miss. By combining cellular automata with a linear global dynamic, we allow learning of overall nonlinear emergent behavior while still being able to **derive interpretable linear approximations (Koopman eigen-dynamics) at a high level**.
* We sum the local and global contributions to get the total change `dx`. Then we update the state: `x_next = x + dx`. Here, we included a nonlinearity (`tanh`) to keep the latent values in a reasonable range (this mimics how NCAs often squash or clip states to avoid divergence).

This **NCADynamics** module will be instantiated in our agent. It operates on the latent state tensor (for example, an internal grid representing the environment or agent’s belief of the world). Ensure the module and its parameters are moved to the chosen `device`. For example:

```python
nca = NCADynamics().to(device)
```

This component forms the **core latent transition** mechanism of the agent: each forward pass computes the next latent state given the current one. In a later section, we’ll integrate this into a recurrent loop for the agent’s internal simulation/planning.

## 3. Integrate Wavelet-Aware Latent Representations

To enrich the latent dynamics with multi-scale information, incorporate a **wavelet transform on the latent features**. This will allow the agent to capture both coarse and fine details by explicitly transforming the latent state into frequency components. We will implement this by defining a **wavelet transform module or function** and inserting it into the latent update loop. The goal is to have the latent state updated not only by the NCA+Koopman operations, but also to mix in features extracted via wavelet decomposition of the state.

First, define a small utility that performs a discrete wavelet transform (DWT) on a 2D tensor. For simplicity, consider using a Haar wavelet (which splits data into average and difference components). We can implement one level of Haar wavelet transform along each spatial dimension using PyTorch convolution or pooling operations (ensuring differentiability). For example:

```python
import torch.nn.functional as F

def haar_wavelet_transform(x):
    # x: Tensor shape [B, C, H, W]
    # Define Haar filters (low-pass and high-pass) for 1D convolution
    low_pass = torch.tensor([0.7071, 0.7071], device=x.device).reshape(1,1,-1)
    high_pass = torch.tensor([0.7071, -0.7071], device=x.device).reshape(1,1,-1)
    # Apply filters along width dimension
    # Use F.conv1d by flattening height*batch as batch and width as time
    B, C, H, W = x.shape
    x_reshaped = x.view(B*C*H, 1, W)            # treat each row of each channel as a signal
    low_coeff = F.conv1d(x_reshaped, low_pass, stride=2)
    high_coeff = F.conv1d(x_reshaped, high_pass, stride=2)
    # low_coeff and high_coeff shape: [B*C*H, 1, W/2]
    coeffs = torch.cat([low_coeff, high_coeff], dim=1)  # 2 channels per input representing low/high
    coeffs = coeffs.view(B, C, H, -1)  # shape [B, C, H, W/2 * 2channels] effectively [B, 2*C, H, W/2]
    # Now apply transform along height (transpose roles of H and W for reuse)
    coeffs = coeffs.permute(0,1,3,2)  # shape [B, 2*C, W/2, H]
    coeffs = coeffs.contiguous()
    B2, C2, W2, H2 = coeffs.shape
    coeffs_reshaped = coeffs.view(B2*C2*W2, 1, H2)
    low_coeff_h = F.conv1d(coeffs_reshaped, low_pass, stride=2)
    high_coeff_h = F.conv1d(coeffs_reshaped, high_pass, stride=2)
    coeffs2 = torch.cat([low_coeff_h, high_coeff_h], dim=1)  # [B2*C2*W2, 2, H/2]
    coeffs2 = coeffs2.view(B2, C2, W2, -1)  # shape [B, 2*C, W/2, H/2 * 2channels] => [B, 2*C, W/2, H]
    coeffs2 = coeffs2.permute(0,1,3,2)      # back to [B, 2*C, H/2, W/2]
    return coeffs2  # Wavelet coefficients (approximation and detail at one level)
```

This function splits the input image (latent feature map `x`) into four sub-bands: LL (low frequency both horizontally and vertically), LH, HL, HH (mixed high/low frequencies). We achieved this by convolving with low-pass and high-pass filters along width and height. The output has effectively doubled the channel count (the factor of 2 from width filtering and factor of 2 from height filtering), while halfing the spatial resolution in each dimension. The operation is fully differentiable since it’s composed of PyTorch convolutions. We will use this to produce **wavelet-transformed latents**.

Next, integrate this into the NCA update loop. One strategy is to **augment the latent state with wavelet features before feeding into the NCA convolution**. For example, modify `NCADynamics.forward` to compute wavelet coefficients of `x` and concatenate them as additional channels for local update:

```python
class NCADynamics(nn.Module):
    # ... (same init as before, maybe adjust channel counts for wavelet)
    def forward(self, x):
        B, C, H, W = x.shape
        # Compute wavelet coeffs for current state
        coeffs = haar_wavelet_transform(x)           # shape [B, 2*C, H/2, W/2]
        # Upsample back to original resolution for concatenation
        coeffs_upsampled = F.interpolate(coeffs, size=(H, W), mode='bilinear', align_corners=False)
        # Concatenate wavelet coefficients with original state along channels
        x_aug = torch.cat([x, coeffs_upsampled], dim=1)  # now has C + 2*C = 3*C channels
        # Apply local update convolution on augmented channels (adjust conv filter accordingly)
        dx_local = self.update_conv(x_aug)
        # ... (global Koopman update as before)
        dx = dx_local + dx_global
        x_next = x + dx
        x_next = torch.tanh(x_next)
        return x_next
```

Here we expanded the `update_conv` input channels to accommodate the augmented input (if original channels = C, and wavelet gave 2*C, then update\_conv’s in\_channels = 3*C). The **wavelet-transformed latent (`coeffs`) provides frequency-domain features** that the local update can use. By upsampling it to the original spatial size and concatenating, we allow the convolution to mix standard spatial features with multi-scale frequency features at each location. This way, the NCA’s local rule becomes **wavelet-aware**, detecting patterns not just in raw activations but also in their coarse vs. detail components. This technique follows the idea from Tang *et al.* (2024), where they feed both original signals and wavelet-transformed signals into a neural network, resulting in improved feature learning and accuracy. In our context, the latent state’s **low-frequency (LL) content carries global structure** while high-frequency (HH) content carries fine details; the NCA can use both to decide on the state update.

After computing the local update `dx_local` on augmented channels, we perform the global Koopman update `dx_global` as before (note that `dx_global` is computed from the original latent `x_flat` – one might also experiment with including wavelet info in the Koopman projection, but here we keep it simple by using original state for global dynamics). We then combine the two updates. The result `x_next` thus has integrated multi-scale adjustments: **local rule updates informed by wavelet features and a global linear trend**.

Finally, ensure that **all new parameters** (like increased conv weights, etc.) are on the correct device and that the tensor ops (like `haar_wavelet_transform`) use the same device. Our `haar_wavelet_transform` uses device of input for filter creation, which is correct. This integration means each forward pass of `NCADynamics` now does substantially more computation (two conv passes for wavelet), but remains end-to-end differentiable. The wavelet transform will **re-enter the core latent loop at every time step**, providing fresh frequency-domain context for the NCA’s decision making at each iteration. In summary, the latent update is now **augmented with a differentiable wavelet transform**, enabling the model to capture patterns across scales (coarse layout vs. detailed residuals) within the single unified update step.

## 4. Implement Complex-Valued Associative Memory Module

Next, introduce a **differentiable associative memory** to store and recall latent patterns, utilizing complex-valued representations for rich capacity. We create a class (subclassing `nn.Module`) that maintains a memory of certain key patterns (could be past states, prototypes, goal states, etc.) and can retrieve a memory given a query (e.g. the current latent state). The module will use **complex-valued tensors** for the memory storage to leverage phase relationships – PyTorch supports complex tensors (dtype `torch.cfloat`) since version 1.7, so we can perform complex linear algebra directly.

Define the memory class like so:

```python
class ComplexMemory(nn.Module):
    def __init__(self, memory_size=10, vector_dim=128):
        super(ComplexMemory, self).__init__()
        # Initialize memory matrix with complex values
        # Use real and imag parts from normal distribution
        real = torch.randn(memory_size, vector_dim)
        imag = torch.randn(memory_size, vector_dim)
        self.memory = nn.Parameter(torch.view_as_complex(torch.stack((real, imag), dim=-1)))
        # The memory is a trainable parameter matrix of shape [memory_size, vector_dim] in complex form
    def forward(self, query):
        # query: complex tensor of shape [vector_dim] or [batch, vector_dim]
        # Ensure query is complex
        if not torch.is_complex(query):
            # If query given as real tensor, convert to complex by treating it as real part
            query = query.to(self.memory.dtype) + 0j
        # Compute similarity between query and each memory item (complex inner product)
        # For batch queries, do matrix multiplication
        # Conjugate the memory for proper complex inner product
        similarity = torch.matmul(query, self.memory.conj().T)  # shape: [batch, memory_size] complex
        # Use magnitude of similarity to decide weights (soft retrieval)
        weights = F.softmax(similarity.abs(), dim=-1)           # shape: [batch, memory_size] real weights
        # Reconstruct by weighted sum of memory patterns
        recalled = torch.matmul(weights, self.memory)           # shape: [batch, vector_dim] complex
        return recalled
```

In this implementation:

* We initialize `self.memory` as a trainable complex matrix of shape `(M, D)` where `M = memory_size` (number of patterns that can be stored) and `D = vector_dim` (the dimensionality of each pattern, which we might set equal to the latent vector size or some compressed size). We form it by sampling real and imaginary parts from a normal distribution and using `torch.view_as_complex` to combine them. The use of `nn.Parameter` makes it part of the model’s learnable parameters, so it can be tuned during training. This draws inspiration from Hopfield networks where the weight matrix encodes stored patterns, but here we explicitly maintain memory items rather than a fully dense weight matrix.
* The `forward` method takes a `query` vector (which could be the current latent state or some key derived from it). We ensure the query is complex (`torch.is_complex(query)` check). If the latent state is real, we convert it to complex by treating it as the real part and adding zero imaginary. Then we compute **similarity between the query and each memory item**. We do this via a matrix multiplication: `query * memory^H` (where memory^H is the conjugate transpose). If `query` is size `[D]` (single vector), it will be automatically expanded to `[1, D]` for the matmul with the memory shape `[M, D]^H = [D, M]`. The result is a complex similarity score for each of the `M` memory entries. We take the **magnitude (`abs()`) of these complex similarities and apply a softmax** to get a normalized weight for each memory entry. This means memory entries whose complex phase aligns well with the query (i.e., high inner product magnitude) get higher weight. Finally, we reconstruct a recalled vector as the weighted sum of memory items (`weights * memory`). The output `recalled` is a complex vector of dimension `D`. In effect, this module acts like a content-addressable memory: it retrieves something close to whichever stored pattern best matches the query. The usage of complex numbers allows patterns to **interfere and superpose** in ways not possible with purely real weights – e.g., phases can cancel out, which might help with capacity or encoding of cyclic features.

Now, consider how this memory integrates with the predictor. We likely want to use it to recall relevant latent features from past experiences or predefined prototypes to aid the current state update. One strategy: at each time step, after the NCA+Koopman update, use the updated latent as a query to memory, and then **inject the recalled vector back into the latent state** (or into the network producing actions). Concretely, suppose our latent state from the NCA is shape `[B, C, H, W]`. We could flatten or pool it to a vector of length `D` (matching memory’s vector\_dim), query the memory, and then merge the recalled vector back. For example:

```python
# Inside agent's step update (not necessarily within NCADynamics, could be in Agent class)
latent = nca(latent)  # update latent via NCA+Koopman+wavelet
# Derive a query from latent for memory:
query_vec = latent.view(latent.shape[0], -1)      # flatten entire latent grid per batch
# (Optionally reduce dimension via a linear layer to match memory vector_dim)
query_vec = query_vec @ W_q # if needed, where W_q is a learned projection matrix of size [C*H*W, D]
mem_out = memory_module(query_vec)                # complex retrieval, shape [B, D] complex
# Integrate memory output back into latent:
# One approach: add the real part back to latent (assuming latent is real-valued)
mem_out_real = torch.view_as_real(mem_out)        # get tensor of shape [B, D, 2] representing real+imag
mem_out_real = mem_out_real[...,0]                # take real part of recalled vector
# Expand or reshape this to the latent shape for integration
mem_out_expanded = mem_out_real.view(B, C, H, W)
latent = latent + mem_out_expanded               # add retrieved pattern to latent state (residual addition)
```

This code assumes `vector_dim = C*H*W` for simplicity (so that memory outputs match flattened latent). In practice, we might choose a smaller `vector_dim` to reduce parameters and learn a projection (represented by `W_q` above) from the high-dimensional latent to the memory key space. The integration shown simply adds the recalled pattern into the latent (one could also concatenate or gate it). Adding it means the latent state gets nudged toward stored familiar patterns whenever the memory finds a match. This mimics how a Hopfield network would converge a noisy pattern toward a stored prototype, except here it’s one step and done via a learned retrieval rather than iterative energy minimization. We can draw an analogy: in a classic Hopfield network, the weight matrix is the sum of outer products of patterns and updating the state via \$x\_{t+1} = \text{sign}(W x\_t)\$ causes it to recall stored memories. In our setup, the memory module explicitly does a soft retrieval of the closest stored pattern.

Important details:

* **Complex dtype handling**: We ensure the memory is stored as `torch.cfloat` and computations (matmul) happen in complex domain (PyTorch supports this in linear algebra ops). The `.abs()` yields a real tensor of similarity magnitudes for softmax. The gradient flows through these operations into both the memory entries and the query. So the memory can learn to store useful patterns during training (the gradient from the loss will update `self.memory` entries).
* We took only the real part of the recalled vector to add to the latent. This is a design choice – since our latent state was real (assuming the NCA state is a real tensor), we need to convert the complex memory output to a real correction. One could also imagine splitting the latent’s channels into real and imaginary parts and keeping the whole pipeline complex-valued, but that would complicate other components. Using the real component of memory recall as a corrective term is a simple way to integrate it. We could also concatenate it as extra channels instead of adding.

This **ComplexMemory module** should be instantiated in the main script and passed into the agent. E.g., `memory_module = ComplexMemory(memory_size=50, vector_dim=latent_dim).to(device)`. When building the overall agent forward pass (next sections), ensure to call `memory_module` at the appropriate time to inject its effect. The presence of an associative memory allows the system to **retain information over long timescales** and to **recognize familiar latent configurations**, which is crucial for an active-inference agent that must remember goals or past observations. The complex values potentially increase storage capacity by using the phase dimension to reduce interference between stored items. Throughout training, this memory could learn to store prototypical latent states (for example, latent encodings of target goals or common scenarios) and thus improve the agent’s ability to quickly infer actions when those scenarios occur.

## 5. Design JEPA-Style Joint Embedding Predictive Training

Now that the model’s architecture (latent dynamics and memory) is in place, we set up the **Joint Embedding Predictive Architecture (JEPA)** style training mechanism. The idea is to train the network by making it predict parts of its input (or future states) in latent space, using a contrastive embedding loss rather than direct pixel-wise losses. This will encourage the agent to learn a compact abstract representation that captures predictive relationships. Specifically, we will implement a training objective where the model must embed a context and a target such that the embeddings are **closer for true context-target pairs** and farther for mismatched (negative) pairs, in line with contrastive learning principles.

First, define the components needed for JEPA: typically, a **context encoder** \$f(x)\$, a **target encoder** \$g(y)\$, and a predictor \$h(f(x))\$ that tries to match \$g(y)\$ for the true pair. In our case, the latent state produced by the NCA (possibly after memory integration) can serve as the context encoding for time \$t\$, and the latent state at a future time (or a different part of input) will be the target. To implement this cleanly, we can incorporate two parallel sub-networks or simply reuse the same network in a time-shifted manner (the latter is more parameter-efficient, akin to using the same model for context and target with different inputs). For clarity, let's create two modules: `encoder` and `predictor`. The `encoder` will map an observation (e.g., the PyBullet state or image) to a latent embedding \$z\$. The `predictor` will take the current latent \$z\_t\$ and output a predicted embedding \$\hat{z}*{t+k}\$ for some future step \$t+k\$. We then compare \$\hat{z}*{t+k}\$ with the actual encoded latent \$z\_{t+k}\$ using a contrastive loss (like InfoNCE or simplified cosine similarity objective).

Implement simple versions:

```python
class Encoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Encoder, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, latent_dim)
        )
    def forward(self, x):
        return self.net(x)

class PredictorHead(nn.Module):
    def __init__(self, latent_dim):
        super(PredictorHead, self).__init__()
        # A simple MLP to predict future embedding from current embedding
        self.net = nn.Sequential(
            nn.Linear(latent_dim, latent_dim)
            # We could add more layers or nonlinearity if needed
        )
    def forward(self, z):
        return self.net(z)
```

Here, `Encoder` might be used to encode both context and target observations (e.g., taking as input a state vector or some processed observation). If our latent state from NCA is already an encoding of observation, we might not need a separate encoder for JEPA – instead, we can treat the latent state as \$f(x)\$. But we might still define an `Encoder` if the raw observation is high-dimensional (like image pixels) to initially get into latent space. In an active inference setting, often the internal model itself acts as the state encoder. For flexibility, we include it. The `PredictorHead` is a lightweight network that takes the current latent and outputs a predicted latent. We intentionally keep it simple (maybe just a linear layer or identity) because our NCA already does heavy lifting of predicting next state; the predictor head could be used to predict a *target embedding at a different time scale or a different modality (like predicting an unseen part of input)* as per JEPA.

Now the **training objective**: a contrastive loss. One approach is **InfoNCE**: we want the predicted embedding \$\hat{z}*{t+k}\$ to be close (in cosine similarity or L2) to the true embedding \$z*{t+k}\$ of the future observation, and far from embeddings of other (negative) samples. We can implement a simplified version using cosine similarity and a margin. Another approach is to use something like **Barlow Twins or VICReg** style loss to make the embeddings of positive pairs similar and decorrelate others. For clarity, let's do a cosine similarity approach:

```python
def contrastive_loss(pred, target, temperature=0.1):
    # pred, target: tensors of shape [batch, latent_dim]
    # Compute normalized cosine similarity
    pred_norm = F.normalize(pred, dim=1)
    target_norm = F.normalize(target, dim=1)
    # Positive cosine similarity (diagonal of pred vs target)
    pos_sim = (pred_norm * target_norm).sum(dim=1)  # batch of cosines
    # We need negatives: use other samples in batch as negatives (typical InfoNCE)
    # Compute full cosine similarity matrix between pred and target
    sim_matrix = torch.matmul(pred_norm, target_norm.T)  # shape [batch, batch]
    # Apply temperature and softmax for InfoNCE
    sim_matrix /= temperature
    # For each sample i, compute InfoNCE loss:
    # - log( exp(sim[ii]/T) / sum_j exp(sim[ij]/T) )
    batch_size = pred.size(0)
    labels = torch.arange(batch_size, device=pred.device)
    loss = F.cross_entropy(sim_matrix, labels)
    return loss
```

This `contrastive_loss` treats each pair \$(pred\_i, target\_i)\$ in the batch as a positive, and \$(pred\_i, target\_j)\$ for \$j \neq i\$ as negatives. It uses cross-entropy formulation of InfoNCE: the correct “class” for `pred_i` is `target_i` among all target embeddings in the batch. Minimizing this makes \$\hat{z}*{t+k}\$ align with the true \$z*{t+k}\$ (maximize their cosine similarity), while minimizing similarity to others. This captures the JEPA idea of comparing two inputs (here current vs future) and learning their relationship without requiring a generative pixel output. Notably, **JEPA avoids direct pixel prediction and works in latent space**, which is exactly what we do by operating on embeddings.

During training, for each step, we would:

* Take an observation at time \$t\$ (context) and a later observation at \$t+k\$ (target).
* Compute context latent: either feed observation through `Encoder` then through NCA steps for \$k\$ iterations, or simply through NCA \$k\$ steps starting from the encoded state at \$t\$ (depending on how we design the predictive usage).
* Compute target latent: feed the observation at \$t+k\$ through `Encoder` (and possibly no NCA, as it's the true observed future).
* Use `PredictorHead` on the context latent (or last latent after \$k\$ NCA updates) to get \$\hat{z}\_{t+k}\$.
* Compute loss = contrastive\_loss(\$\hat{z}*{t+k}\$, \$z*{t+k}\$).
* Backpropagate to update parameters of `Encoder`, `PredictorHead`, and crucially the NCA and memory (since the NCA was used to produce the context latent at \$t+k\$). In practice, we might unroll the NCA for \$k\$ steps and compute \$\hat{z}\_{t+k}\$ after that – this is like the predictor implicitly being the NCA itself. Alternatively, use the predictor as a one-step head to project current latent to future latent.

For simplicity, one could even set \$k=1\$ (predict next step). But JEPA can be used for masked modality too. For example, in a static scenario, pick different parts of an input image as context vs target (like I-JEPA for masked image patches). In our agent context, predicting future state is most relevant. So \$k=1\$ step prediction in latent space via NCA is analogous to model rollout, and training that with contrastive loss is akin to energy-based or self-supervised temporal consistency learning.

By training this way, the agent’s internal model learns to produce latent representations that are useful for predicting what comes next, rather than just reconstructing input. It avoids naive generative losses and focuses on **latent similarity of true vs predicted futures**, which improves noise tolerance and compactness of representation. Implementation-wise, integrate this in the training loop (section 9) by sampling transitions from the simulation or replay buffer, and computing the loss as above. The structures `Encoder`, `PredictorHead`, as well as the main NCA (and possibly memory) will get optimized with this loss. We might also include additional losses like reconstruction or reward prediction if needed, but JEPA’s contrastive loss would be primary for self-supervised world model learning. As LeCun’s paper noted, JEPA can be trained via masking or contrastive criteria like VICReg/Barlow Twins; here we effectively implement a form of it with InfoNCE on temporal predictions.

## 6. Embed a Differentiable Symbolic DSL for Logic and Constraints

To incorporate high-level knowledge and constraints in a structured way, we integrate a **differentiable Domain-Specific Language (DSL)** component. This means we allow certain rules or programs (written in a mini-DSL) to influence the agent’s computations, and crucially, this influence is differentiable – gradients can flow from these symbolic rules into the neural network parameters. In practice, we can achieve this by writing a small interpreter that parses symbolic expressions and executes corresponding PyTorch operations.

**Design the DSL**: Define a set of symbols or instructions that represent useful operations or constraints for the agent. For example, for an active inference robot, we might have DSL commands like `MOVE(X)` (desired movement to location X), `AVOID(Y)` (avoid region Y), or logical conditions like `IF obstacle THEN turn`. Each such instruction will be associated with a PyTorch-compatible operation or loss. For simplicity, let's consider a DSL that defines *constraints on state/action* and *objectives*. We could write rules like:

* `goal(x_target, y_target)` – symbolic goal position that the agent should move toward.
* `stay_inside(x_min, x_max, y_min, y_max)` – a constraint keeping the agent within some region.
* `face_direction(theta)` – a desired orientation.

We then implement these as differentiable components. One straightforward way is to compile these instructions into a **loss term or a modification of the network output**. For example, `goal(x_t,y_t)` could translate to a loss measuring distance between agent’s current position (a part of state) and \$(x\_t, y\_t)\$. We ensure this loss is differentiable w\.r.t. the state (which is output of the network’s internal simulation of state).

Let's illustrate by implementing a simple parser for such symbolic commands, and integrating with the agent’s forward. Consider a global variable or object `dsl_program` that holds a list of instructions (for instance, provided by user or scenario). We interpret them in code:

```python
# Example DSL program representation (could be a list of tuples)
dsl_program = [
    ("goal", 5.0, 5.0),
    ("stay_inside", 0.0, 10.0, 0.0, 10.0)
]

def evaluate_dsl(latent, state):
    # latent: the agent's internal latent state (torch tensor)
    # state: the agent's actual or predicted physical state (e.g., position) as torch tensor
    total_loss = torch.tensor(0.0, device=state.device)
    for instr in dsl_program:
        if instr[0] == "goal":
            _, goal_x, goal_y = instr
            # Suppose state contains position at indices [0,1]
            dx = state[0] - torch.tensor(goal_x, device=state.device)
            dy = state[1] - torch.tensor(goal_y, device=state.device)
            dist = torch.sqrt(dx*dx + dy*dy)
            # Add loss for distance to goal (to be minimized)
            total_loss = total_loss + dist
        elif instr[0] == "stay_inside":
            _, xmin, xmax, ymin, ymax = instr
            # Penalize being outside [xmin,xmax]x[ymin,ymax]
            px, py = state[0], state[1]
            penalty = torch.relu(px - torch.tensor(xmax, device=px.device))
            penalty += torch.relu(torch.tensor(xmin, device=px.device) - px)
            penalty += torch.relu(py - torch.tensor(ymax, device=py.device))
            penalty += torch.relu(torch.tensor(ymin, device=py.device) - py)
            total_loss = total_loss + 10.0 * penalty  # weight the constraint higher
        # Additional instructions handled similarly...
    return total_loss
```

In this snippet:

* We loop through each instruction. For `goal(x,y)`, we calculate the Euclidean distance between the agent’s position `(state[0], state[1])` and the goal `(goal_x, goal_y)`. This distance is added as a loss term. Minimizing this loss (via gradient descent in either network parameters or even directly in actions, see active inference below) will cause the agent to move toward the goal.
* For `stay_inside(xmin,xmax,ymin,ymax)`, we compute a penalty if the agent’s position is outside the specified bounds. We use `torch.relu` to accumulate how far outside the boundary the agent is (zero if inside). This penalty is weighted (10.0 here) and added to the loss. Minimizing this will push the agent to remain within the region.

Because these operations (`sqrt`, `relu`, arithmetic) are PyTorch tensor ops, the resulting `total_loss` is differentiable w\.r.t. `state`. If `state` in turn is an output of our neural network (or a function of network outputs), gradient will flow into the network encouraging it to satisfy these symbolic constraints. This effectively makes the DSL a source of **differentiable constraints** or objectives integrated into training or planning.

To integrate with model training, we could add `evaluate_dsl(latent, predicted_state)` to our loss function (e.g., combined with the JEPA loss or any reinforcement learning reward). For instance, the overall loss might be `loss = contrastive_loss + λ * dsl_loss`, where λ is a weight balancing the influence of symbolic rules. One could also integrate the DSL in the agent’s action selection: e.g., during each simulation step, adjust the predicted next action by optimizing the DSL objectives via gradient descent on action parameters (that is an **active inference approach** where actions are treated as latent variables optimized to minimize a free-energy-like objective). In such a case, the DSL-provided loss acts like a part of the “free energy” to minimize with respect to action choices. For example:

```python
# Pseudocode for active inference action optimization
action_params = torch.randn(action_dim, requires_grad=True, device=device)
optim = torch.optim.SGD([action_params], lr=0.1)
for _ in range(10):  # perform a few gradient steps on action
    predicted_next_state = model_dynamics(current_state, torch.tanh(action_params))
    loss = evaluate_dsl(None, predicted_next_state)
    optim.zero_grad()
    loss.backward()
    optim.step()
# Then apply the optimized action_params as the action
action = torch.tanh(action_params).detach()
```

In the above, we treat `action_params` as a free variable and optimize it to minimize the DSL loss (assuming `model_dynamics` predicts next state from current state and an action, akin to an internal model). The gradients flow through `evaluate_dsl` into `action_params`. This demonstrates how a symbolic objective (like goal position) can directly influence action selection in a differentiable manner – a core idea of active inference (choosing actions that minimize expected free-energy or surprise).

Furthermore, our DSL can be extended with more complex constructs, e.g. conditionals or loops, but to keep differentiability, we must define them in a smooth way. A conditional `IF condition THEN penalty` can be implemented as a soft condition (using continuous functions instead of non-differentiable ifs). For instance, `IF obstacle THEN turn` could be a differentiable multiplication of the obstacle presence (0/1 expressed via a sigmoid from sensor reading) with a turning encouragement term. The blog or research on differentiable programming suggests similar patterns: TensorFlow, PyTorch, DiffTaichi are themselves considered differentiable DSLs for certain domains. In fact, DiffTaichi is a DSL for physics that is fully differentiable. Our approach mirrors that idea by making a mini-language for agent logic that compiles to PyTorch operations.

In code integration: parse the DSL program at runtime (could be hardcoded for now). Then in each simulation step or training batch, after obtaining the agent’s predicted state or latent, call `evaluate_dsl`. Use the resulting loss as:

* Additional term in the training loss (to gradually teach the network to respect constraints),
* Or as a direct optimisation objective for action inference (as shown above).

Thus, the **differentiable DSL** provides a mechanism to inject prior knowledge or desired behavior (like staying within bounds or reaching a goal) into the learning process in a controlled way, without breaking differentiability. The overall single-file script contains this parser and evaluation function, ensuring that even high-level logic remains part of the autograd graph.

## 7. Enforce KKT Constraints in the Optimization Loop

When training or executing the agent, we often need to enforce hard constraints (physical limits, safety requirements, or equality conditions). To handle these systematically, integrate a **Karush-Kuhn-Tucker (KKT) based constraint enforcement** in the optimization process. This means that instead of (or in addition to) simply penalizing constraint violations, we explicitly introduce Lagrange multiplier terms and perform updates that satisfy the KKT conditions at optimum.

In practice, we can implement a simple version of an **augmented Lagrangian method** inside our training loop or as part of a custom optimizer. The strategy: maintain a learnable (or updatable) multiplier for each constraint and add a term to the loss of the form \$\lambda \* g(x)\$ (for equality constraint \$g(x)=0\$) or \$\lambda \* h(x) + \eta \max(0,h(x))^2\$ (for inequality \$h(x)\le0\$) etc., and update \$\lambda\$ alongside network weights. The KKT conditions tell us that at optimum, the gradient of the Lagrangian must be zero and constraints satisfied. We can mimic gradient ascent on \$\lambda\$ while doing gradient descent on network parameters to approach feasibility.

**Implementation steps**:

1. **Define constraint functions** relevant to the agent. For example, an equality constraint might be something like ensure total energy is conserved, or the sum of certain latent components equals 1. An inequality might be something like joint angles within bounds. Let's say we have a simple equality constraint \$c(x) = 0\$. We will enforce it with Lagrange multiplier.

2. **Initialize Lagrange multipliers** (as torch tensors, possibly with `requires_grad=False` since we update them manually rather than via autograd, typically). For example: `lambda_c = torch.tensor(0.0, device=device)`. If multiple constraints, use a tensor or list of lambdas.

3. **Modify the loss** during training: add \$\lambda\_c \* c(x)\$ to the loss (for equality constraints) and a penalty term \$\frac{\alpha}{2} c(x)^2\$ to ensure convergence (this is an augmented Lagrangian approach). For inequality \$h(x) \le 0\$, one common approach is adding \$\lambda\_h \* \max(0,h(x))\$ and a penalty on \$\max(0,h(x))^2\$.

4. **After computing gradients on network parameters**, update the multipliers with gradient ascent: \$\lambda\_c := \lambda\_c + \beta \* c(x).detach()\$. We use `.detach()` to stop gradient (since we don’t want second derivative influence on lambda update, it's an explicit update). The update increases the multiplier if the constraint is violated positive (for equality, any nonzero), thereby in next iteration the loss term \$\lambda\_c \* c(x)\$ will push \$c(x)\$ toward opposite sign to reduce violation.

Here’s a concrete snippet integrated in the training loop:

```python
# Assume we have a function constraint_func(x) that returns a tensor for equality constraint
lambda_c = torch.zeros(1, device=device, requires_grad=False)  # Lagrange multiplier

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
for data in training_loader:
    # Forward pass
    output = model(data)
    # Compute primary loss (e.g., JEPA contrastive or RL loss)
    base_loss = compute_base_loss(output, data)
    # Compute constraint value
    c_val = constraint_func(output)
    # Augmented Lagrangian loss part
    constraint_loss = lambda_c * c_val + 0.5 * gamma * (c_val**2)
    total_loss = base_loss + constraint_loss
    # Backpropagate on model parameters
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    # Update Lagrange multiplier (gradient ascent)
    lambda_c += alpha * c_val.detach()
    # Optionally clamp lambda for stability or enforce sign (for inequality lambda >=0)
    lambda_c = torch.clamp(lambda_c, min=0.0)
```

In this code:

* `constraint_func(output)` defines our constraint. Suppose as an example the constraint is that the agent’s predicted next state `output` must have a particular linear momentum value (just as a placeholder example). If this returns, say, `-0.5` (meaning it’s below desired), the Lagrange term will adjust.
* We added to the loss: `lambda_c * c_val` which comes from the Lagrangian \$\lambda \* c(x)\$, and a quadratic penalty \$0.5 \* gamma \* c\_val^2\$ which helps ensure convergence by penalizing large violations (this is the Augmented Lagrangian term). `gamma` is a hyperparameter to tune (like penalty weight).
* After optimizing model parameters, we update `lambda_c` by adding a fraction of the current constraint violation (`alpha * c_val`). This is a gradient ascent on the dual variable aiming to satisfy the dual feasibility condition. Essentially, if \$c(x)\$ is positive (constraint > 0), \$\lambda\$ increases so next time the model is pushed harder to reduce \$c(x)\$; if \$c(x)\$ is negative, \$\lambda\$ might decrease.
* We detach `c_val` to avoid backprop through this update – we treat it as an external update, not part of autograd graph.
* If it were an inequality constraint \$h(x) \le 0\$, we would also ensure \$\lambda\_h >= 0\$. The code shows clamping \$\lambda\_c\` to min 0.0 if needed (i.e., we ensure multipliers don’t go negative, in line with KKT condition that \$\lambda \ge 0\$ for inequality constraints).

By doing this, we aim to fulfill the KKT optimality conditions: at optimum, \$c(x)=0\$ and the gradient of parameters includes \$\lambda \* \nabla c(x)\$ which will vanish if \$c(x)=0\$ and \$\lambda\$ has adjusted appropriately. In essence, the training process treats constraint satisfaction like finding a stationary point of the Lagrangian (which is what KKT solutions are). This approach has been used in differentiable optimization layers as well: for instance, differentiating through QP solvers relies on KKT conditions. We are manually incorporating a simpler version of that idea, effectively solving a constrained problem by turning it into an unconstrained one (Lagrangian) and satisfying the conditions through alternating updates (this is related to methods of multipliers or ADMM if we extended it).

In a live simulation loop (not just training), if we want to ensure the agent’s chosen actions satisfy constraints (like torque limits or collision-free paths), we could also apply a **projected gradient** step after each action selection. For example, if action \$a\$ must satisfy \$h(a) \le 0\$, we can check after the agent computes an action and if it violates, project it back to the feasible set (set \$a := \text{clip}(a)\$ or similar). This is a simpler alternative to using Lagrange multipliers in real-time. The PyTorch forum suggests doing exactly this: enforce constraints after each optimizer step by projection. However, using the Lagrange multiplier method can have smoother behavior since it gradually encourages constraint satisfaction rather than hard clipping, and it integrates nicely with gradient descent.

By updating our training loop or control loop with these steps, we ensure constraints are respected to within tolerance, effectively embedding domain knowledge (like physics limits) into the learning. The single-file implementation can hold these updates in the same loop that computes losses – since it's pure Python/PyTorch code, it remains self-contained. Over time, the multiplier \$\lambda\_c\` will converge to a value that enforces \$c(x) \approx 0\$, and the model parameters will settle in a state that satisfies the constraint while optimizing the primary objective. This is precisely what the KKT conditions guarantee at optimum (stationarity, primal feasibility, dual feasibility, complementary slackness). Our approach here captures stationarity (by mixing gradients) and dual updates (via lambda), fulfilling the spirit of those conditions in a differentiable training setup.

## 8. Develop Real-Time Interface with PyBullet, ImGui, and OpenCV

With the model and training mechanics in place, we now build a **real-time interface** to visualize and interact with the agent in the PyBullet simulation. This involves using **OpenCV for rendering frames** and possibly **ImGui for GUI controls** such as sliders or buttons to adjust parameters on the fly. All of this will be integrated into the single script’s main loop.

**PyBullet Rendering**: Since we connected to PyBullet in DIRECT mode earlier, we don’t have an automatic GUI. We can manually get camera images each step and display them. Use `p.getCameraImage` as illustrated in section 1 (in `get_observation`). We might set up a fixed camera or a camera attached to the agent. For example:

```python
# Set up camera parameters once
view_matrix = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0,0], distance=5.0, yaw=45, pitch=-30, roll=0, upAxisIndex=2)
proj_matrix = p.computeProjectionMatrixFOV(fov=60, aspect=1.0, nearVal=0.1, farVal=100.0)
```

Then each loop iteration:

```python
img_width, img_height = 320, 320
_, _, rgb_img, depth_img, _ = p.getCameraImage(img_width, img_height, viewMatrix=view_matrix, projectionMatrix=proj_matrix)
rgb_array = np.array(rgb_img, dtype=np.uint8).reshape(img_height, img_width, 4)  # RGBA
frame_bgr = cv2.cvtColor(rgb_array, cv2.COLOR_RGBA2BGR)
cv2.imshow("Simulation", frame_bgr)
cv2.waitKey(1)  # necessary to update image window
```

This will show a window "Simulation" with the current camera view. Using OpenCV (`cv2.imshow`) in a loop achieves a simple real-time display. The `waitKey(1)` call allows the window to refresh and also can capture keyboard input (if needed for manual override or quitting).

**ImGui Controls**: To adjust parameters (like toggling training/pause, slider for exploration noise, etc.), integrating ImGui in Python can be done via libraries like **pyimgui** or **DearPyGui**. For example, using pyimgui: we need to initialize an OpenGL context and create an ImGui renderer. This is somewhat involved, but broadly:

```python
# Setup ImGui context
imgui.create_context()
impl = gl_imgui.FixedPipelineRenderer()  # or appropriate renderer for the windowing system
```

We would then have an event loop where we start a new ImGui frame, create windows and widgets, and render them. For example, inside the main loop:

```python
imgui.new_frame()
if imgui.begin("Controls"):
    _, training = imgui.checkbox("Training", training_enabled)
    training_enabled = training  # toggle training on/off
    changed, lr_val = imgui.slider_float("Learning Rate", learning_rate, 1e-5, 1e-2)
    if changed:
        optimizer.param_groups[0]['lr'] = lr_val
        learning_rate = lr_val
    imgui.text(f"Step: {global_step}")
imgui.end()
# Render ImGui on top of OpenCV window if possible
imgui.render()
impl.render(imgui.get_draw_data())
```

This snippet creates a GUI window with a checkbox to turn training on/off and a slider to adjust the learning rate, as well as a text display of current step. The variables `training_enabled` and `learning_rate` are assumed to be defined. Changing them through GUI immediately affects the model’s behavior (because we apply the changes to the optimizer). The ImGui frame is drawn over the existing OpenGL context. If using OpenCV window, one approach is to create an OpenGL window for ImGui and possibly combine the rendering (OpenCV’s `imshow` creates its own window which is not directly compatible with ImGui).

A workaround is to manually draw the camera image as a texture in an ImGui window instead. PyImgui can create image textures from raw data using e.g. `impl.texture_rgba_polygons` or using OpenGL directly. For brevity, suppose we skip deep integration and use a simpler approach: use OpenCV window for visual, and a separate small ImGui window for controls (since pyimgui can create its own window). This means running two event loops (one for cv2, one for ImGui). Alternatively, use a high-level library like **DearPyGui** which can show images and GUI in one window easily (but that’s another dependency).

In a single-file prototype, the simplest might be: use OpenCV for display and keyboard input (like pressing 'p' to pause training, 'q' to quit), and avoid ImGui if it's too heavy. However, since the requirement explicitly mentions ImGui, we’ll assume advanced users can set up the context. The **ImGui integration ensures we can interact with the agent live**, adjusting things or observing internal values. For example, we could display the agent's internal latent state as an image (if it’s 2D grid). We could create an ImGui window showing a heatmap of one channel of the latent by converting that tensor to a CPU numpy array and using `cv2.imshow` in a small window. Or using ImGui's `Image` widget if we manage to create a GPU texture. Those details aside, the key is that the main loop will contain calls to process GUI events and show images.

**Bringing it together**: We set up a loop like:

```python
global_step = 0
training_enabled = True
while True:  # main simulation loop
    # 1. Get observation from environment
    obs = get_observation()  # e.g., camera image as tensor
    # 2. Agent forward pass: encode obs, update latent via NCA, retrieve memory, decide action
    latent = encoder(obs.view(1, -1))  # encode observation to latent
    # (latent should be shaped appropriately for NCA e.g., [1,C,H,W] if representing an image)
    latent = latent.view(1, init_channels, H, W)
    # run a few NCA iterations internally
    for _ in range(planning_steps):
        latent = nca(latent)
        # Optionally use memory:
        mem_out = memory_module(latent.view(1, -1))
        mem_correction = torch.view_as_real(mem_out)[...,0].view_as(latent)
        latent = latent + mem_correction  # integrate memory
    # 3. Decode latent to action or state prediction
    predicted_state = decode_latent_to_state(latent)  # e.g., a small network or direct mapping
    action = policy_network(latent)  # if a policy net produces action directly
    # 4. Apply action in PyBullet
    apply_action_to_sim(robot_id, action)
    p.stepSimulation()
    # 5. (Optional) If training_enabled, compute JEPA loss using predicted_state vs actual next state
    if training_enabled:
        next_obs = get_observation()
        with torch.no_grad():
            target_latent = encoder(next_obs.view(1,-1)).detach()
        pred_latent = predictor_head(latent.view(1,-1))
        jepa_loss = contrastive_loss(pred_latent, target_latent)
        # Add any DSL constraint losses:
        dsl_loss = evaluate_dsl(latent, predicted_state)
        total_loss = jepa_loss + dsl_loss
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        # Update Lagrange multipliers if any (per section 7)
        lambda_c += alpha * constraint_func(predicted_state).detach()
    # 6. Visualization updates
    # Render camera view via OpenCV (already done by get_observation if using camera, but we might have separate wide-angle cam for display)
    _, _, rgb_img, _, _ = p.getCameraImage(img_width, img_height, viewMatrix=view_matrix, projectionMatrix=proj_matrix)
    frame = np.array(rgb_img, dtype=np.uint8).reshape(img_height, img_width, 4)
    cv2.imshow("Simulation", cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR))
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break  # exit loop on 'q'
    # Update ImGui interface
    imgui.new_frame()
    if imgui.begin("Agent Info"):
        imgui.text(f"Step: {global_step}")
        imgui.text(f"JEPA loss: {float(jepa_loss.item()):.4f}")
        # perhaps display some latent stats or toggle buttons
        _, training_enabled = imgui.checkbox("Training Enabled", training_enabled)
    imgui.end()
    # Render ImGui on screen (assuming an OpenGL context behind cv2, this part is tricky)
    imgui.render()
    impl.render(imgui.get_draw_data())
    global_step += 1
```

This pseudo-code inside the loop shows how all components tie together in real-time:

* We get observations, feed through the model (encoder -> latent dynamics -> memory -> predictor).
* We obtain an action and apply it to the simulation.
* If training mode is on, we do a training step: get the new observation, compute loss, backpropagate to update model weights (both the world model and perhaps policy if any).
* We also enforce DSL constraints and update multipliers accordingly.
* We render the simulation view and update the GUI with current information and controls.
* We allow breaking out of the loop with a key press.

All these occur sequentially in one thread. The GUI updates should ideally be done in the same thread due to OpenGL context; mixing cv2 and ImGui can complicate rendering. If needed, one could separate concerns by having ImGui window for everything (including rendering the simulation image in an ImGui image widget). For brevity, this outline demonstrates both outputs.

Finally, ensure proper cleanup at script end: disconnect PyBullet (`p.disconnect()`), destroy OpenCV windows (`cv2.destroyAllWindows()`), etc., to release resources when the loop exits.

This interface allows us to **observe the agent's behavior and training progress live**. We can see the environment, adjust whether it’s learning or not, and monitor losses and constraint values. The immediate feedback and interactivity are invaluable for debugging such a complex system in real time. Moreover, it's all in one script: the same code that defines the model also runs the simulation and UI, making it easy to run and iterate.

## 9. Final Integration and Execution in a Unified Script

Having detailed each component, we now combine them into a single cohesive script. The final script’s structure will be: imports and configuration (Section 1), class definitions for NCA, Memory, etc. (Sections 2, 4, etc.), the DSL evaluation function (Section 6), instantiation of all components (NCA model, memory, encoders, etc.), the main loop (Section 8) which includes the training (Section 5) and constraint updates (Section 7), and the visualization interface (Section 8).

Key considerations for the unified design:

* **Order of definitions**: Since it’s one file, define classes and functions before they are used. For instance, define `NCADynamics`, `ComplexMemory`, `Encoder`, `PredictorHead`, and any other modules at the top (after imports). Define utility functions like `get_observation`, `evaluate_dsl`, `constraint_func` next. Then instantiate PyBullet and the models.

* **Device placement**: Right after instantiating each nn.Module (NCA, Memory, etc.), call `.to(device)`. Also, when converting observations or other data to tensors, specify `device=device`. This ensures GPU is utilized. For example:

  ```python
  nca = NCADynamics(...).to(device)
  memory_module = ComplexMemory(...).to(device)
  encoder = Encoder(input_dim, latent_dim).to(device)
  predictor_head = PredictorHead(latent_dim).to(device)
  ```

  And in `get_observation`, we already convert to tensor on device. Similarly, ensure any constant tensors in DSL or constraint calculations are moved to device (we did `torch.tensor(val, device=state.device)` inside `evaluate_dsl` for that reason).

* **Gradients across modules**: Because everything is PyTorch, as long as we combine losses properly and call backward, autograd will handle computing gradients through NCA, through memory (including complex operations which autograd supports), through encoders, etc. There is no part of our pipeline that is non-differentiable (ImGui/OpenCV are outside the computational graph). Even the PyBullet environment is not differentiable, but we don’t need to backprop through it; we only use it for data collection.

* **Active inference note**: In our design, active inference-like behavior can emerge if we allow internal gradient-based optimization for actions as shown. We could either let a learned `policy_network` output actions, or do the manual gradient optimization of `action_params` each step to minimize a combination of predicted free-energy (e.g., prediction error + DSL losses). For completeness, a simple approach is to have an `nn.Linear` that maps latent to action (policy) and train it with a standard RL or imitation objective if available. However, demonstrating the gradient-based action selection at least once (as above with `action_params`) shows the system’s differentiable nature. In real-time, we might choose one approach or the other. If using the gradient approach, include that in the loop (perhaps when `training_enabled` is False, we try planning via gradient for a few iterations to refine action).

* **Ensuring real-time performance**: Our model includes convolutional updates and matrix multiplications; on a modern GPU these should be fine for moderate sizes (e.g., 16x16 latent grid, 16-32 channels). PyBullet can usually step at decent rates if not rendering too high resolution. If the loop is too slow, one can reduce image size or iterations. We might run the NCA `planning_steps` a few times per control step (like a small internal simulation horizon). This effectively makes the agent plan ahead a bit internally (if `planning_steps>1`).

Now we verify coherence: the final system **actively infers** by using its internal model (NCA + memory) to predict outcomes and choose actions that fulfill objectives. It's **fully differentiable**: the entire loop (except the physics stepping) is differentiable, and we used that for training and could use it for action refinement. It's all in one script without external dependencies beyond those needed.

To illustrate an example run: imagine the agent has a goal (5,5). The DSL adds a loss driving it to (5,5). The agent’s internal model sees current state, predicts forward (with NCA), memory might recall previously seen good trajectories to goal, and the policy or gradient descent computes an action direction. That action is applied, moving the agent a bit toward the goal. Meanwhile, JEPA contrastive training ensures the latent dynamics become better at predicting actual next states, so over time the model’s predictions (and thus chosen actions) improve to minimize surprise. Constraint via KKT (if any, say stay\_inside) prevents it from going out of bounds. The GUI shows the robot moving and allows toggling training or adjusting a parameter if needed in real-time.

Finally, to finalize the script, ensure to handle program termination properly:

```python
cv2.destroyAllWindows()
p.disconnect()
```

at the end of loop. And that’s it – running this Python script should launch the simulation and agent logic.

By following this comprehensive checklist and implementation guide, we have a single Python file that defines and interconnects:

* a **Koopman-augmented NCA** world model,
* **wavelet-based latent feature augmentation**,
* a **complex-valued associative memory** for pattern completion,
* a **JEPA-inspired contrastive training objective** for self-supervised learning,
* a **differentiable symbolic DSL** to encode goals and constraints,
* a **KKT-driven optimization step** ensuring those constraints are respected,
* and a real-time **PyBullet simulation loop with an interactive OpenCV/ImGui interface**.

Each piece is placed in context (class definitions or training loop) where it directly contributes to the overall active-inference-capable agent. The result is an **implementation-ready** (albeit prototype-level) single-file PyTorch system that one can run to observe an agent learning and acting in a simulated environment, with all advanced techniques integrated seamlessly.&#x20;
