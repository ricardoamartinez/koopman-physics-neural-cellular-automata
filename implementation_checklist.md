Understood. I’ll compile a fully refactored and comprehensive technical checklist integrating all previously discussed specifications — including Koopman-based NCA, complex-valued wavelet-aware latents, unitary associative memory, differentiable symbolic DSL, real-time JEPA loss, KKT constraint enforcement, and PyBullet with ImGui/OpenCV interface — with unambiguous, implementation-ready detail.

I’ll let you know once it’s ready for review.


# Complete Implementation Checklist for Integrated PyTorch System

## 1. Koopman-Based Neural Cellular Automaton (NCA)

* [ ] **Input & Latent Setup:** Accept a 480×480 RGB input (e.g. from the simulator) and initialize the NCA’s complex latent state `L` of shape `[B, C, H, W]` (batch, channels, height, width). Ensure most of the input is zero (sparse) except for active regions. Map the RGB input into the latent (e.g. copy or encode it into certain latent channels) while other latent channels start as zero. Use `torch.view_as_complex()`/`view_as_real()` to represent `L` as complex (two real channels per complex channel) so that subsequent complex operations are supported by autograd.

* [ ] **Skew-Hermitian Generator `A`:** Define a learnable generator matrix `A` ∈ ℂ<sup>C×C</sup> to govern latent dynamics. Enforce `A` to be skew-Hermitian (i.e. `A^H = -A`) at all times. For example, parametrize `A` via a real matrix `P` and update `A = (P - P^T) * i` each step so that `A + A^H = 0`. This ensures `A` lies in the Lie algebra of the unitary group. Register this operation in the autograd graph (or use PyTorch’s parametrization utilities with `orthogonal_map="cayley"` for automatic enforcement).

* [ ] **Cayley Transform to Unitary `U`:** Compute the discrete Koopman update matrix `U` by applying the Cayley transform to `A`. Formally, `U = (I - A) * (I + A)^{-1}`, which yields a unitary matrix when `A` is skew-Hermitian. Implement this by solving `(I + A) * U = (I - A)` for `U` (e.g. use `torch.linalg.solve` or `torch.linalg.inv` on `(I + A)`). This `U` acts as a linear evolution operator in the latent space, preserving the norm of latent vectors (unitary dynamics).

* [ ] **Convolutional Message Passing (Local Update):** Implement the local NCA update rule as a convolutional message-passing step. Use a 3×3 (or appropriate) convolution `Conv` that operates on the complex latent `L` to collect neighbor information for each cell. Because PyTorch doesn’t natively support complex convolution, handle this by splitting into real and imaginary parts: e.g. have two real-valued convolution kernels `(W_real, W_imag)`, and compute `Conv(L) = (W_real * L_real - W_imag * L_imag) + i*(W_real * L_imag + W_imag * L_real)` to convolve complex feature maps. Ensure padding and stride are set so that the output is the same 480×480 size. Apply an activation function if needed (e.g. ReLU on real/imag separately) to model non-linear cell interactions.

* [ ] **Koopman Evolution Update:** Update the latent state by combining local and global dynamics. For each NCA step, first compute the local convolution update ΔL = `Conv(L)` (complex). Then apply the global unitary transform: `L_new = U · L + ΔL`. Here `U · L` means a matrix multiply of shape `[C×C]` on the channel dimension of `L` at every spatial location (this mixes the latent channels globally in a rotation-like manner). Use PyTorch’s complex operations (or `view_as_real` to perform the batched matrix multiply) so that gradients propagate through `U`. The addition `+ ΔL` integrates the local perturbation. This two-part update ensures **local** neighbor interactions as well as **global** Koopman linear evolution in latent space.

* [ ] **Iteration and Stability:** Iterate the NCA update for each simulation tick (or multiple internal steps per tick if needed). Maintain stability by possibly using small step sizes in `A` (if treating it as continuous generator) or by adjusting the convolution output scale. The unitary part `U` by design preserves norm, so monitor the effect of the additive convolution term on latent norms; if necessary, renormalize or clip the latent to prevent explosion. Ensure the entire update (`Conv` and multiply by `U`) is wrapped in a `torch.nn.Module` so its parameters (`W_real`, `W_imag`, and `A` via `P`) are tracked and updated through autograd.

* [ ] **Gradient Flow:** Verify that gradients flow from any downstream loss through the NCA: e.g. if using a loss on the latent or on an output image reconstructed from latent, check that `A` (via `P`) and the conv filters get non-zero grads. Special care: if using any custom complex operations (like manual conv), test them on a small input to ensure `tensor.real` and `tensor.imag` operations are properly used (PyTorch’s `torch.view_as_complex` can convert a `[*,2]` trailing dimension into a complex view for certain ops).

## 2. Wavelet Embedding & Compression

&#x20;*Example of a 2D wavelet transform decomposing an image into multi-scale frequency sub-bands. The input image is split into one low-frequency approximation (LL) and several high-frequency detail components (LH, HL, HH) at each level. Such wavelet decompositions (Haar, Daubechies, etc.) are invertible, enabling lossless reconstruction from compressed coefficients. We leverage this to compress latents and guide attention across scales.*

* [ ] **Integrate PyTorch Wavelets:** Import and initialize the wavelet transform modules from `pytorch_wavelets`. Use `DWTForward` for forward discrete wavelet transform and `DWTInverse` for inverse transform. Ensure the wavelet filters include **Haar** (db1), **Daubechies-4** (db4), and wavelet packet options. For example, set `wave = 'haar' or 'db4'` when constructing `DWTForward` to select the filter. Verify that these transforms run on the GPU (the library uses PyTorch tensors, so `.to(device)` should move them).

* [ ] **Wavelet Decomposition of Latent:** Before or after certain network stages, perform a wavelet decomposition of the latent tensor. Decide on the level of decomposition (e.g. 1 level or multilevel). For each forward transform, you’ll get approximation coefficients and detail coefficients (e.g. for level-1: LL, LH, HL, HH). Ensure the shape of coefficients makes sense (pytorch\_wavelets returns a tuple of `(YL, YH)` where `YL` is low-frequency and `YH` is a list of high-frequency subbands). Use these in the autograd graph (the transform is differentiable).

* [ ] **Latent Compression:** Implement compression by discarding or quantizing certain wavelet coefficients. For instance, you might zero out some high-frequency bands or apply a learned thresholding to coefficients. Because the wavelet transform is invertible, even aggressive compression can be recovered in reconstruction. Maintain differentiability: if using a threshold, use a soft-threshold (continuous approximation) or ensure it’s implemented as a PyTorch operation so gradients flow (e.g. parameterize a mask or use a smooth shrinkage function).

* [ ] **Multiscale Attention Routing:** Use the multi-scale wavelet representation to inform attention mechanisms in the model. For example, compute the energy or variance of each sub-band patch and use it to modulate attention weights. High-energy wavelet coefficients indicate important fine details; focus the NCA’s or memory’s updates on those regions. Implement this by creating an attention map: upsample low-res coefficient maps (like LL) to full size and combine with high-frequency maps to get an importance map. Then, weight updates or network outputs by this map (e.g. as a gating). This effectively provides a **multi-scale prior** that guides the model to prominent features.

* [ ] **JEPA Projection in Wavelet Domain:** Provide an option to perform contrastive prediction in the wavelet domain. This means instead of comparing or predicting latents in the original spatial domain, transform latents (or images) into wavelet coefficients and perform the Joint-Embedding Predictive Architecture comparisons there. Implement a projection head that takes latent `L` and produces its wavelet coefficient vector (perhaps flattened or pooled) to be used in InfoNCE loss. By operating on wavelet features, the model’s InfoNCE can emphasize structural differences at various frequencies. Make this togglable (e.g. a boolean `use_wavelet_head` that chooses between direct latent vs. wavelet projection for the contrastive module).

* [ ] **Invertibility and Reconstruction:** Whenever wavelet transforms are used for intermediate processing, ensure you invert them to get back to the main latent stream. For example, after compressing or attending via wavelet, use `DWTInverse` to reconstruct the modified latent to the original spatial shape before feeding it to subsequent modules. Test that an identity (no-op) compression followed by inverse yields the original latent (numerically verify a few entries) to confirm correctness. Any slight discrepancy can break the end-to-end training since the model might learn to exploit reconstruction errors, so it’s important the transform is accurately inverted (the library assures perfect reconstruction given the coefficients).

* [ ] **Wavelet Packet Decomposition (Advanced):** If full wavelet packet transform is needed (i.e. decomposing not just the low-pass but also high-pass recursively), implement this by multiple passes of `DWTForward` on the detail bands. For example, to perform a wavelet packet, apply `DWTForward(level=1)` to get LH/HL/HH, then apply another level of DWT on each of those sub-bands. Organize the coefficients in a tree structure. Because this can explode the number of coefficients, ensure the channel count in latent is sufficient or apply only in a small region. Verify differentiability by backpropagating through a dummy packet transform and inverse (since it’s composed of standard DWT ops, it should be fine).

## 3. Complex-Valued Associative Memory

* [ ] **Memory Structure:** Implement a Hopfield-like associative memory that stores complex-valued patterns. Let the dimension of each pattern be `N_p` (likely related to latent patch size or channel count). Use a matrix `M` ∈ ℂ<sup>N\_p×N\_p</sup> to represent the memory, or equivalently store a list of stored patterns `{p_k}`. Initialize `M` as the zero matrix (no stored patterns initially) or an identity if using it as a baseline projector. Ensure that `M` (or the pattern list) is maintained across time steps in the simulation (it’s part of the system’s state, not a learnable weight – though it changes via Hebbian updates).

* [ ] **Hebbian Write (Outer Product Rule):** When a new pattern `p` (complex vector of length N\_p) needs to be stored (e.g. a “latent patch” from a particular region/time that we want to remember), perform a Hebbian update: $M := M + \eta \, p \, p^H$, where $p^H$ is the Hermitian transpose (conjugate transpose) of `p`, and η is a scaling factor (could be 1 for simple Hopfield, or a small value if we want gradual updates). This outer product update is O(N\_p²) per pattern, but effectively O(N\_p \* K) if K patterns stored (since each pattern adds one outer product). If storing explicitly as a list, you can skip forming the full matrix and instead accumulate the list of patterns for use in retrieval. Make sure to use complex operations for the outer product (PyTorch supports `torch.outer` or `torch.matmul` on complex tensors). After each write, if capacity K is exceeded, decide on a replacement strategy (e.g. remove the oldest pattern or the one with least similarity to current state).

* [ ] **Unitary Projection Normalization:** After each write, normalize the memory to lie on the “unitary projection” manifold. In practice, this means ensuring that retrieving from the memory doesn’t blow up norms and that stored patterns don’t interfere destructively. One approach is to orthonormalize the set of stored pattern vectors. For example, perform a Gram-Schmidt process on `{p_k}`: for a new pattern, subtract projections on all existing orthonormal basis vectors and then normalize it to unit length before adding. This way, each stored pattern is orthogonal to others and has norm 1. If using the matrix `M`, this orthonormal set yields $M = \sum_k p_k p_k^H$ which is an idempotent Hermitian projector (its eigenvalues are 0 or 1). Implement this normalization carefully: use `torch.qr` or `torch.linalg.svd` on the complex matrix if needed (ensure to handle complex dtypes; PyTorch’s SVD works for complex). This step keeps `M` on a constrained manifold, which aids stable recall.

* [ ] **Complex Query & Read:** Implement the memory retrieval via complex inner products. When a query vector `q` (complex, dimension N\_p) is presented, compute its similarity with each stored pattern or directly multiply by the memory matrix. Two equivalent methods: (a) **Matrix form:** Compute `y = M * q` which yields a retrieved vector (a weighted sum of stored patterns, projecting q onto the subspace of memories). (b) **Pattern list form:** Compute coefficients $\alpha_k = \langle q, p_k \rangle$ for each stored pattern (where the inner product $ \langle q, p_k \rangle = q^H p_k$ is complex). Then output $y = \sum_k \alpha_k p_k$. In either case, use `torch.matmul` or `torch.einsum` to perform the complex dot products (PyTorch will handle complex conjugation in the dot if you use `@` or `torch.matmul` on one vector as `q.conj().T @ p_k`). The retrieved vector `y` will be a complex combination of stored patterns, where phases of $\alpha_k$ indicate the alignment of `q` with pattern `p_k`. If `q` exactly matches a stored pattern (phase aside), ideally `y ≈ p_k` (Hopfield fixed-point retrieval).

* [ ] **Phase-Coded Associative Recall:** Leverage the phase information in complex vectors for more nuanced queries. For instance, you can encode certain attributes in the phase of pattern entries (a form of holographic or resonator memory). Upon querying, interpret the complex inner product magnitude as match strength and the phase as a cue for any shift/rotation needed. Implement a mechanism to extract the best match: e.g. find `k = argmax |α_k|` (the pattern with highest overlap magnitude) and optionally rotate `y`’s phase to align with that pattern’s phase. This can be done by dividing out the phase of α\_k from y or similar, if needed for downstream use.

* [ ] **Memory Retrieval Usage:** Feed the retrieved vector `y` into the downstream modules to guide prediction. For example, in the JEPA contrastive module or the action policy, include `y` as additional input. Concretely, you might concatenate `y` (or some function of it) with the current latent `L` before the predictor network, so that the network can utilize past pattern info. Another use: if the memory indicates a high match (i.e. current latent is similar to a past latent `p_k` which we know led to a certain outcome), you could bias the model’s output toward the outcome associated with `p_k`. This can be achieved by a learned readout that takes both the normal model features and memory recall features. Design the integration such that gradients can flow into the memory usage (though note: the memory content update is non-differentiable Hebbian, so you won’t backprop into past patterns, but you can backprop through the retrieval process into the query `q`).

* [ ] **Capacity Management:** Because a Hopfield memory has limited capacity (classic Hopfield \~15% of N\_p patterns for robust recall), decide on a maximum K. Implement memory decay or removal to avoid indefinite growth: e.g. use FIFO removal (pop oldest pattern when full) or remove the pattern with smallest |α| overlap on a new query to focus on relevant memories. Alternatively, use a learned forgetting: multiply `M` by a factor <1 periodically to gradually remove influence of old memories. Document and tune this as needed in experiments (e.g. bouncing ball may need very short memory, while block stacking might need longer memory of past moves).

## 4. JEPA-Style Contrastive Prediction Module

&#x20;*Joint-Embedding Predictive Architecture (JEPA) concept: A context encoder produces an abstract state representation $s_x$ from the current observation, and a target encoder (or ground truth encoder) produces $s_y$ from a future or masked observation. A predictor network is trained to map $s_x$ to a prediction $\hat{s}_y$ that estimates the future latent. The training uses both regression (to make $\hat{s}_y$ close to $s_y$) and contrastive objectives (to ensure $s_y$ is more similar to $\hat{s}_y$ than to negatives).*

* [ ] **Context & Target Encoding:** Define how to obtain the context latent $s_x$ and target latent $s_y$ for the prediction task. Typically, $s_x$ can be the model’s current latent state (or a projection thereof), and $s_y$ can be the latent encoding of the **future** state we want to predict (e.g. latent at the next time-step or a few steps ahead). If predicting a masked portion, $s_y$ might be the latent of the **current** observation but with some parts hidden. Implement a function to generate training pairs $(s_x, s_y)$: e.g. for each time `t`, let $s_x = \text{latent}(t)$, and $s_y = \text{latent}(t + \Delta)$ for some prediction horizon Δ (or $s_y = \text{latent}(t)$ of a masked image region). Ensure that obtaining $s_y$ does not leak information (e.g. if using future, ensure our model doesn’t already have it).

* [ ] **Predictor Network:** Design a prediction model $f_{\theta}$ that takes $s_x$ (and possibly additional inputs like memory retrieval or actions) and outputs a predicted latent code $\hat{s}_y$ of the same dimension as $s_y$. This could be a simple MLP or a convolution (if spatial structure is preserved) or even just a linear layer if $s_x$ already has rich features. If $s_x$ and $s_y$ are spatial (e.g. feature maps), you might use a small UNet or convolutional head to predict future feature map. Initialize this predictor with appropriate size (input dim = dim(s\_x), output dim = dim(s\_y)). Keep it as a PyTorch module so parameters are learned online.

* [ ] **Reconstruction Loss (L2):** Compute an L2 loss between $\hat{s}_y$ and the actual $s_y$. This is a direct regression loss ensuring the predictor outputs are close in Euclidean sense. Use `torch.nn.MSELoss` or manually $\| \hat{s}_y - s_y \|^2$. If $s_y$ is complex, define the loss on real and imaginary parts (or take absolute difference). This loss alone trains the predictor to approximate the future latent, but can lead to blurry predictions if the task is uncertain (hence the need for contrastive loss as well).

* [ ] **InfoNCE Contrastive Loss:** Implement an InfoNCE-based contrastive loss to handle the prediction as a classification among negatives. For a given anchor (context) $s_x$ and its positive future $s_y$, sample a set of negative examples $\{ s_y^{-} \}$ that are **incorrect futures**. These negatives can be drawn from other sequences in the batch (temporal negatives) and other examples in the batch (batch negatives). For instance, if you have `B` sequences, use each sequence’s actual future as negative for the others. Construct the similarity score between $s_x$ and any $s_y'$ as $\text{sim}(s_x, s_y') = \frac{s_x \cdot s_y'}{\|s_x\|\|s_y'\|}$ (cosine similarity, or dot product if magnitudes are already normalized by training). Then for each positive pair, formulate the loss: $\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(s_x, s_y)/\tau)}{\exp(\text{sim}(s_x, s_y)/\tau) + \sum_{\text{negatives}} \exp(\text{sim}(s_x, s_y^{-})/\tau)}$, where τ is a temperature hyperparameter. Implement this by computing similarity scores for one positive and N-1 negatives and feeding into a softmax. Use a reasonable τ (e.g. 0.1 or 0.2) and allow it to be adjusted.

* [ ] **Negative Sampling Strategy:** Ensure the negatives are challenging and diverse. Use both **across time** and **across batch** negatives: e.g. for context at time *t* in sequence *i*, use the true future of sequence *j≠i* at time *t* as a negative (batch negative), and potentially use other time steps from sequence *i* (like a far past or a randomly chosen other time) as negatives as well. You could maintain a queue of past latent codes (like MoCo-style) so that even if batch size is small, you have more negatives from previous iterations. Implement the logic to gather negatives: perhaps concatenate all $s_y$ in the batch into a matrix and use all but the correct one as negatives for a given $s_x$. Make sure to exclude the positive’s index when computing the softmax denominator for each sample.

* [ ] **Masking in Frequency Domain:** When configured, apply masking to the latent in the wavelet domain as a form of data augmentation or targeted prediction. Two possible modes: (a) **Mask input to predictor:** Drop certain frequency bands of $s_x$ or $s_y$ (e.g. zero out all high-frequency coefficients) to force the predictor to focus on complementary information. For example, mask out low-frequency components of $s_y$ – the predictor then must guess the overall structure from details. (b) **Define prediction targets in freq domain:** e.g. predict only the masked coefficients of $s_y$. Implement this by transforming $s_y$ via DWT, zeroing a subset of sub-bands, then inverting back to get a partially-masked target. The predictor tries to predict this partial information. This effectively creates a harder task that prevents trivial pixel-wise predictions and encourages learning semantic features. Provide a parameter to toggle frequency masking and perhaps a schedule (e.g. increase masking ratio over time as model improves).

* [ ] **Loss Combination and Training:** Combine the losses from above. For each training iteration (which could be each time-step or each small batch of time-steps): compute $\mathcal{L}_{MSE} = \| \hat{s}_y - s_y \|^2$ and $\mathcal{L}_{InfoNCE}$ as derived. Normalize them if needed (sometimes InfoNCE is on a different scale; you might just sum them assuming comparable scale, or introduce a weighting factor). The total loss $\mathcal{L} = \mathcal{L}_{MSE} + \lambda_c \mathcal{L}_{InfoNCE}$ (choose λ\_c \~1.0 as a starting point). Backpropagate this total loss. Ensure that the gradient indeed flows into the predictor network parameters and the context encoder (if it’s just the main model latent, it will flow into earlier layers like NCA weights). Verify also that negatives that are drawn from other sequences do not receive positive reward (they shouldn’t, but double-check indexing in implementation).

* [ ] **Consistency Check:** During training, periodically check that the predictor is doing something sensible: for instance, take a context state, predict $\hat{s}_y$, and compare it not only to the true future but also qualitatively inspect if $\hat{s}_y$ looks like a plausible future state (e.g. if latent can be decoded to an image, decode and see if it has the object moved in the correct direction). Also monitor the InfoNCE component: the probability of the positive in the softmax (it should increase over time, meaning the model is getting better at distinguishing the correct future from negatives).

## 5. Active Inference Loop with KKT Dual Updates

* [ ] **Unified Online Learning Loop:** Set up the main loop such that at each time-step the system both **acts/thinks** and **learns** (no separate training phase). This means after every simulation step (or after a short trajectory), you will perform an update of all model parameters using the observed prediction errors. Implement this as an **online gradient descent**: e.g. use an optimizer (Adam or SGD) that you call `.step()` on at each time-step using the losses from the JEPA module (and any other losses you might include, e.g. reward prediction). Because this is online (non-i.i.d data, continuous adaptation), consider using a relatively low learning rate and maybe an optimizer with momentum to smooth updates.

* [ ] **Identify Constraints:** List out the constraints in the system that need to be enforced or encouraged:

  * *Unitary dynamics:* The generator matrix `A` should remain skew-Hermitian so that `U` is strictly unitary (constraint: $A + A^H = 0$). Also the convolution in NCA might be constrained (e.g. maybe we want it norm-preserving in some sense, or at least not divergent).
  * *Memory normalization:* If the memory matrix is meant to be an orthonormal projector, ideally $M^2 = M$ and $M^H=M$ (idempotent and Hermitian). After our orthonormalization step, these hold, but we might treat any deviation as constraint violation.
  * *Latent norm preservation:* Perhaps we desire that certain latent variables maintain a norm (since unitary U preserves norm, we might want conv updates not to change global norm too much or enforce |latent| after update equals |latent| before for stability).
  * *Action constraints:* If there are physical constraints (like not exceeding motor limits), those could be considered too (though these might be handled in the simulator itself).

  Write these out as mathematical constraints $g_i(\theta) = 0$ or inequalities if any. For example, $g_1(A) = A^H + A = 0$.

* [ ] **Augmented Lagrangian Setup:** Formulate a Lagrangian $\mathcal{L}(\theta, \lambda) = \mathcal{L}_{\text{task}}(\theta) + \sum_i \lambda_i \, h_i(\theta) + \frac{\rho}{2} \sum_i \|h_i(\theta)\|^2$ for each equality constraint $h_i(\theta)=0$. For our case, an example constraint term could be $h_1(A) = A^H + A$ (which should be 0). In code, you can compute a penalty term e.g. `constraint_loss = ||A + A.conj().T||_F^2` as a measure of unitary deviation. Do similar for other constraints if needed (like $||M^2 - M||_F^2$ for memory, or simply ensure we orthonormalize memory directly instead of via gradient). Introduce Lagrange multiplier coefficients `lambda_i` for each constraint (initialize them, e.g. as zeros). In practice, because we want to enforce these during training, you might add to the loss a term `+ lambda_i * h_i + 0.5 * rho * h_i^2` (where multiply and square denote Frobenius norm or sum of squares) to push the system toward satisfying constraints.

* [ ] **Gradient Computation with Constraints:** Each iteration, compute gradients of the augmented Lagrangian with respect to parameters. PyTorch autograd can directly handle the added penalty terms. For example, adding `constraint_loss` as above will add gradients pushing `A` toward skew-Hermitian. The linear λ term will also contribute constant pushes when there’s a violation. If constraints are exactly satisfied, their grad contribution vanishes. Make sure to include these in the loss that's backpropagated. Monitor the size of these gradients relative to task gradients to ensure the constraints are being enforced without completely overwhelming the task (tune ρ if needed).

* [ ] **Dual Variable (λ) Updates:** After updating the primal parameters (e.g. weights of NCA, predictor, etc.), update the Lagrange multipliers to enforce the constraints (this is the “dual ascent” step in KKT). Use a simple update: $\lambda_i := \lambda_i + \mu \, h_i(\theta)$, where $h_i(\theta)$ is the current constraint violation and μ is a small step size. Implement this in code by computing the value of each constraint function after the parameter update and adding a fraction of it to the corresponding `lambda_i`. For example, if `A_dev = A + A.conj().T` (a Hermitian matrix measuring skew-H deviance), update `lambda_unitary += mu * A_dev` (for matrix λ) or perhaps the Frobenius norm of it if you treat λ as scalar. If treating λ as scalar for each constraint, you could use the trace of $A^H A_dev$ as a measure. Ensure these multipliers are stored and persist across iterations. Over time, this should drive the average constraint violation toward zero (if the problem is feasible).

* [ ] **Projection/Repair Step:** In addition to (or instead of) Lagrange penalty, it can help to explicitly project onto constraint manifolds after each update. For example, after each gradient step on `A`, enforce skew-Hermitian by symmetrizing: `A <- 0.5*(A - A^H)`. This is a hard projection that ensures no drift in unitary constraint (which might be easier than relying purely on penalty). Similarly, re-orthonormalize memory patterns after each update cycle (we already do after each write). This hybrid approach (soft constraint via penalty + hard projection) can maintain constraints without slowing learning too much. Make sure to perform such projections *after* using the values for gradient updates, to not break the gradient computations. (In PyTorch, if you do it in `torch.no_grad()` after `loss.backward()` and before `optimizer.step()`, that’s safe.)

* [ ] **Dynamic Step Size Control:** Implement a mechanism to adjust the learning rate (or step sizes μ, ρ) on the fly to maintain stability. For instance, monitor a moving average of the loss and constraints:

  * If the prediction loss is increasing rapidly or diverging, or if any constraint violation is blowing up (e.g. ||A^H + A|| gets large), that’s a sign the step might be too large. Have a condition to reduce the learning rate of the optimizer (e.g. multiply by 0.5 or so) and/or increase ρ (penalty strength) to enforce constraints more strongly.
  * Conversely, if constraints are very well satisfied (almost 0) and the model is learning stably, you might gently increase learning rate to speed up adaptation.
  * Use a scheduler or custom logic: e.g. every 100 steps, if loss > (previous\_loss + tolerance), lr := lr \* 0.7. Alternatively, use PyTorch’s ReduceLROnPlateau with the appropriate metric.
  * For dual variables μ updates: if you find the dual updates overshoot (causing oscillation in constraint satisfaction), reduce μ. If constraints are not improving, increase μ gradually.

  Test the system by introducing a deliberate constraint violation (e.g. set `A` randomly non-skew for a moment) and see if the dual updates bring it back. The KKT-based updates plus projection should keep the system’s internal dynamics stable and consistent with theoretical priors (unitary, etc.) in the long run.

* [ ] **No Train/Eval Split:** Confirm that at no point the code assumes a separate training vs evaluation phase. For example, ensure dropout or batchnorm (if any used) are in a consistent mode (you might prefer to avoid batchnorm due to non-i.i.d. data or use it with a running mean; dropout could be used to inject noise if desired, but treat it as always-on if so). The model should be continuously learning from the incoming data stream. Implement mechanisms to prevent catastrophic forgetting: since it’s always training on the latest data, it might forget earlier skills. Techniques include: a small replay buffer of past data or slowly decaying learning rates. However, these are optional; the primary goal is that at every time step, gradients are applied and the model is *never* frozen waiting for an offline training stage.

## 6. Differentiable Symbolic DSL Module

* [ ] **DSL Grammar Definition:** Define a domain-specific language that is Turing-complete, inspired by lambda calculus or Lisp-like syntax. Choose a simple, flexible syntax (S-expression is a good choice due to ease of parsing). For example, you could support expressions like `(op arg1 arg2)` for binary operations, lambda abstractions like `(lambda (args) (body))`, function application, conditionals `(if cond then_expr else_expr)`, and loops or recursion via self-application. Clearly enumerate the supported primitives: arithmetic operations (`+ - * /`), logical operations, possibly vector/matrix ops for latent, and special keywords for actions (e.g. a symbolic function `move` or `apply_force`). Document this grammar so that you (and the program) know how to parse it.

* [ ] **Parser Implementation:** Write a parser that can take a DSL code (string or list form) and produce an Abstract Syntax Tree (AST) or an equivalent internal representation (like a nested Python tuple structure). If Lisp-like, a simple approach is to leverage Python’s `ast` library if you design the syntax close to Python, or just implement a recursive descent parser since the grammar is simple (for Lisp, you can tokenizes parentheses and symbols and recursively build the tree). Ensure the parser can handle nested expressions and variables. Example: input `"(begin (define x 0) (while (< x 5) (begin (action move_forward) (set x (+ x 1))))"` should produce a structured representation (don’t necessarily implement full Lisp special forms unless needed). At minimum, you need function definitions, function calls, and some way to represent recursion (which could be via named function calls).

* [ ] **Differentiable Interpreter:** Create an interpreter that can execute the AST *in the context of the model’s latent space*. Each primitive operation on latent data must be differentiable (or at least implemented in PyTorch). For example:

  * Arithmetic operations on numbers can be done normally (these might be used for loop counters etc., not critical to differentiate).
  * Operations on latent tensors: define DSL functions like `add(a,b)` that correspond to elementwise tensor addition (`torch.add`), `mul(a,b)` for elementwise multiply, etc. If the DSL needs to manipulate the latent state, provide primitives for that, e.g., a function that returns the current latent or a part of it.
  * If you allow writing to latent via DSL, expose an operation to set or modify latent (this gets tricky differentiability-wise; you might treat it as returning a new tensor rather than in-place modify to keep the computation graph functional).

  The key is that any computation on tensors uses PyTorch ops so gradients propagate. For control flow:

  * **Functional Programming approach:** If using lambda calculus style, you might not have explicit loops but recursion. Direct recursion in a Python interpreter is not differentiable through the control flow (because the number of steps is discrete and can vary). To differentiate through loops, one idea is to unroll them a fixed amount or use a continuous relaxation. However, since the DSL is Turing complete, it can express arbitrary loops – fully differentiating through that is infeasible in general.
  * **Relaxation approach:** For differentiability, you might restrict to a fixed maximum number of steps and backprop through each step (like an RNN unrolling a loop).
  * Alternatively, **straight-through estimator:** run the program discretely and pretend it’s continuous for gradient (not theoretically sound but sometimes used). A safer path is to limit DSL usage in places where exact gradient isn’t needed (e.g. high-level logic) and keep low-level numeric stuff differentiable.

  Implement the interpreter as a function `eval(ast, env)` that evaluates the AST node by node. Make sure that numeric latent ops are done with PyTorch. For branches (`if`), one trick is to avoid hard switching in the gradient: you can evaluate both branches and use a weighted sum with a sigmoid based on the condition as weight (this makes it differentiable through both branches), or simpler: if the condition depends on differentiable data, you might use a soft condition. This is complex, so you might initially restrict conditions to non-differentiable (e.g. based on external input or a threshold on latent that we treat as boolean without grad).

* [ ] **Motor Commands via DSL:** Extend the DSL with primitives that interface with the simulator. For instance, have a symbolic function `(apply-action joint_id torque)` or a higher-level `(move gripper target)` that the interpreter can map to PyBullet calls. For example, define in the interpreter environment a symbol `move_arm` that expects some parameters (like joint target angles or a named pose) and when called, it will output an action tensor for the robot. Since PyBullet action execution itself is not differentiable, treat these DSL functions as **interfacing with the environment** rather than part of the gradient graph. One strategy:

  * DSL program produces a high-level action plan (like a sequence of desired states or a target trajectory).
  * You then convert that plan into low-level motor commands (this conversion could be differentiable if you have a differentiable model of the robot, but here we assume using PyBullet’s physics).
  * Because the environment step isn’t differentiable, you can’t backprop through the real physics. But you can still differentiate through the planning logic if it’s parametric.

  Implement the symbolic-to-motor mapping by maintaining a dictionary of action primitives: e.g., in the interpreter’s `env`, have `'MOVE' : lambda dx: latent_to_action_net(..., dx)`. If the DSL calls `(MOVE 1.0)`, you could interpret that as instructing the agent to move forward by 1.0 units – which you then implement by setting a flag or value that the PyBullet integration will pick up as desired velocity. Make sure these side effects are captured.

* [ ] **Neural Execution Trace:** As the DSL program runs, record the intermediate computations in a way that gradients can flow. For example, if the program calls a sequence of differentiable operations on latent, capture those intermediate latents. You could create a list `trace` and append copies of the latent (or certain variables) at each step. Or if using an unrolled loop, you naturally have those in a list. This trace can be used for analysis or even as additional supervision (for example, if you have an expert program, you could add loss terms that encourage the model’s latent trajectory to match the trace). Ensure not to break the graph: if you store tensors in a Python list, that’s fine – just don’t detach them. If some parts of the program are not differentiable (e.g. control flow decisions), you can still collect the values of relevant differentiable quantities for insight.

* [ ] **Program Synthesis / Generation:** Provide a mechanism to generate or update programs, as the agent might need to alter its strategy. This could be as simple as writing some canonical programs for tasks (hardcoded) or as complex as learning to generate programs with a neural model. Since Turing-complete DSL search is huge, consider simplifying: e.g., define a few parameterized program templates (like a looping strategy for stacking blocks) and just learn the parameters. Alternatively, treat program generation as a separate optimization: maybe use genetic programming or MCTS to find a program that maximizes reward. For integration:

  * You can include a button in the UI to trigger a DSL program search or reload (see UI section).
  * Maintain the current active program in a variable. The interpreter executes it every step (or at key decision points).
  * If using neural guidance: you might have a network that, based on the current latent, outputs a token or line of code (like writing the program on the fly). If so, include that network in autograd and treat the program as a latent plan.

* [ ] **Reward Modeling in DSL:** Incorporate the reward or cost computation into the DSL framework. For example, allow the DSL to query some state and compute a reward signal (like a function in DSL `(reward)` that returns, say, distance of block to goal). Alternatively, after executing a program (or during), evaluate an external reward (from environment, e.g. +1 if block stacked) and feed it into learning. If you want to train the DSL programs themselves (like reinforce which program yields high reward), you could compute policy gradients or use evolutionary strategies. But if you want differentiable training of program selection, you might treat program choice as a continuous parameter (embedding of program) and differentiate through a differentiable surrogate (this is advanced). At minimum, log the reward outcome of the program execution for the agent to learn from (which could inform the meta-learning of program selection or parameter tuning).

* [ ] **Testing the DSL Module:** Verify each component: parse a simple program (e.g. a program that adds two latent vectors), run the interpreter on a dummy latent, and check that the result matches expectation and gradients flow (e.g. if program was “output = latent1 + latent2”, verify that changing latent1 changes output and gradient can propagate back). Test an action sequence program in a static environment scenario (e.g. DSL says move forward, check that the robot moved). Debug any parsing errors or type mismatches (since mixing scalar and tensor). Add ample comments in code for clarity since a one-file solution with a DSL can be complex to follow.

## 7. PyBullet Physics Integration

* [ ] **PyBullet Setup:** Import `pybullet` and connect to the physics server. Use `p.connect(p.DIRECT)` if running without the GUI (since we’ll use our own rendering), or `p.GUI` if you want the Bullet GUI (but that might conflict with our ImGui overlay). Set gravity (`p.setGravity(0,0,-9.81)`) and choose a simulation time step (e.g. `p.setTimeStep(1./240)` for 240 Hz). Load basic environment assets: a ground plane (using Bullet’s plane URDF) for objects to land on, etc.

* [ ] **Environments Initialization:** Create functions or classes for each of the minimal environments:

  * **Bouncing Ball:** Load a sphere (ball) URDF or create a sphere shape via `p.createMultiBody` with a given mass and radius. Set its restitution high (e.g. 0.9) so it bounces. Perhaps drop it from some height. This environment can test the model’s prediction of physics.
  * **Robotic Arm:** Load a simple robot arm URDF (e.g. a KUKA iiwa or Franka Panda, which are in pybullet\_data). Alternatively, use Bullet’s built-in models: `p.loadURDF("r2d2.urdf")` as a stand-in or a simpler 2-link arm if you have one. Position it at a fixed base. Also load some objects for it to interact with (like a cube for block stacking). Ensure to disable GUI controls (unless needed) by `p.resetDebugVisualizerCamera` etc.
  * **Block Stacking:** Use the same arm from above or a separate setup. Load one or two cube URDFs (you can use Bullet’s `cube_small.urdf` or create collision shapes). Place one cube on the ground and another in the arm’s grasp or nearby. The task could be to stack one on the other. Alternatively, if no arm control, block stacking could be a simpler “push blocks with a gripper” scenario.

  Each environment init should return handles or IDs for relevant bodies and joints so that we can apply actions.

* [ ] **State Sensing (RGB and Latent):** For each simulation step, capture the environment state in two forms:

  * **RGB image:** Position a virtual camera in the simulation that can see the scene. Use `p.getCameraImage(width, height, viewMatrix, projMatrix)` to render an RGB image (and depth if needed). For a top-down view, you might place the camera above the scene. Set the resolution to 480×480 to match our model input. Ensure the view covers the area of interest (adjust FOV and distance). Convert the returned image (which comes as a bytes or array) into a NumPy array and then to a PyTorch tensor (and normalize/scale if necessary). This image will be fed as input to the NCA or an encoder. If using the NCA directly on pixels, you might bypass an encoder and directly use image as initial latent. If using an encoder, pass the image to it to get latent.
  * **Latent state via encoder:** Optionally, define a learnable encoder network (CNN) that takes the RGB image (or other low-dimensional state) and outputs a latent vector or map consistent with our latent space. For example, an encoder could be a small ConvNet that outputs a \[C, H, W] tensor of same spatial size (if we want one-to-one mapping) or smaller spatial size that we then upsample. If the environment state is available in structured form (like joint angles, object positions), you can encode those via an MLP to a latent vector and then tile or broadcast into a \[C,H,W] latent map. In either case, integrate this encoder’s parameters into the model so it learns as part of the online loop.
  * Ensure that both forms (image and latent) are exposed to the model: the image mainly for observation, and latent either as identical to initial NCA state or as an additional feature. The prompt suggests state is exposed as both, likely meaning the agent can “see” the RGB and also get some latent representation (like privileged info). You could feed the latent encoder’s output as part of the model’s latent (concatenate with NCA latent channels, for example).

* [ ] **Action Output Mapping:** Create a module to map the model’s output to actual PyBullet actions. Depending on the environment:

  * For the bouncing ball, there may be no action (passive environment), but you could allow the model to, say, apply an impulse or move a paddle. If there’s an action, define what latent -> action means (e.g. one channel of latent could correspond to a force to apply).
  * For the robotic arm, decide on an action space: e.g. target joint velocities or positions. One approach is to have the model output a vector of desired joint angles (dimension = # of joints), or delta angles. Then use `p.setJointMotorControlArray` to move joints toward those angles (position control) each step. Alternatively, output torques and use `p.setJointMotorControlArray(controlMode=TORQUE_CONTROL, forces=…)` – this is more direct but can be unstable; position control is easier. Normalize the model’s output appropriately (e.g. if using tanh, scale to joint limits).
  * For block stacking, if using the arm, same as above. If no arm, maybe the action is to push a block: then action could be 2D force on the block. Adapt accordingly.
  * If the DSL is used to produce high-level actions, you might override the direct latent->action on those steps. But generally, implement a function `apply_actions(latent)` that translates the current latent (or combined latent + memory + DSL outputs) into low-level motor commands. This could be an MLP that reads the latent and outputs continuous values for each action dimension (learned end-to-end). Initialize it to small weights so it doesn’t output large random torques initially. Include this in training so the agent can learn to control effectively (if there’s a reward or intrinsic objective).

* [ ] **Integration with Autograd:** Note that PyBullet itself is outside autograd (we’re not differentiating through physics). So when the model outputs an action and we apply it, there’s no gradient from environment state changes. However, the model can still learn because we provide losses (like predictive losses or eventually reward) that relate to the outcomes. For example, the contrastive predictive module learning world dynamics through latent predictions does not need environment gradients; it treats the environment as producing the next latent to match. So it’s okay that PyBullet is a black box for gradients.

* [ ] **Real-Time Step Loop:** For each tick of the main loop:

  1. Gather observation: render RGB and get latent (encoder or previous latent).
  2. Update model latent via NCA (one or several NCA steps to incorporate the new observation into the latent state, or if the latent is entirely encoded from current obs, use that).
  3. Memory: possibly write the latest latent patch to memory or query memory for guidance.
  4. DSL: if a program is controlling behavior, run the DSL interpreter for one step or decision, which could update latent or produce an action plan.
  5. Action: Compute action vector from latent (and other modules) using the policy mapping. Apply the action in PyBullet with `p.setJointMotorControl...` or similar, then call `p.stepSimulation()` to advance physics by one step (or a few substeps if needed for stability).
  6. Get the new state (for next iteration) and also compute any losses: e.g. use the JEPA module to predict next latent and compare with actual next latent, accumulate loss.
  7. Do online parameter update: backpropagate the loss and update model weights (as per Section 5). If using a small time-step, you might accumulate gradients for a few steps and then update to not overwhelm computation, but per step is fine if stable.
  8. Loop repeats. Use the UI (next section) to handle pausing or adjusting speed in this loop.

* [ ] **Multi-Environment Handling:** You might want to run one environment at a time, but ensure your code structure allows switching between them or resetting. Provide a way to reset an environment to an initial state (e.g. when user wants to restart block stacking scenario). For each environment, possibly maintain separate latent memory (or reinitialize memory on reset). This can be done by checking a variable like `current_env` and calling the appropriate init or reset routines.

* [ ] **Performance Considerations:** Running PyBullet at high speed (240 Hz) with rendering 480×480 can be heavy. To keep real-time, consider:

  * Decrease rendering frequency (you might render at say 30 FPS for the UI, and run physics at higher rate in background without rendering every frame).
  * Use simpler collision shapes and fewer objects to speed up physics.
  * If needed, you can use PyBullet’s synchronous stepping (our loop is already doing that with `stepSimulation()`).
  * Pin your PyBullet simulation to not outrun the model updates – since they’re coupled each tick, it should be fine.
  * Monitor the loop time; if it exceeds real-time (i.e., > \~0.016s per 60Hz frame), you might need to adjust detail (e.g. fewer NCA conv iterations or smaller latent, etc.) to maintain interactive rates.

## 8. ImGui/OpenCV Overlay Interface

* [ ] **Initialize GUI Components:** Use a Python Dear ImGui binding (such as `pyimgui`) or an OpenCV-based UI (`cv2.imshow` for images and trackbars for controls, or the `imgui-datascience` utilities). Set up an OpenCV window for rendering the main simulation view. If using ImGui, create an ImGui context and a window for controls. For simplicity, you might combine OpenCV image display with ImGui by blitting the image into an ImGui image widget, or just have separate OpenCV windows for the image and use ImGui for controls.

* [ ] **Simulation Speed Slider:** Create a slider UI element (ImGui slider or OpenCV trackbar) that ranges, say, 0.1× to 2.0× speed. Tie this to a global variable `sim_speed`. In the main loop, use `sim_speed` to adjust timing: e.g. if using a real-time clock, multiply the sleep or loop delay by (1/sim\_speed) to slow down or speed up. Alternatively, if `sim_speed > 1`, you can call `p.stepSimulation()` multiple times per render frame (speeding up physics), or if <1, insert a `time.sleep` to slow down. Implement it such that at 0 (or a toggle) it effectively pauses (see next item).

* [ ] **Pause/Resume Toggle:** Provide a checkbox or button to pause the simulation. When paused, skip advancing physics and model updates in the loop (but still allow rendering and UI interaction). Implement it by gating the stepSimulation call and model update with a `if not paused:`. Perhaps also add a button "Step" to advance one frame while paused (useful for debugging frame-by-frame). This requires keeping the loop alive but not changing state when paused.

* [ ] **Latent & Memory Visualization:** For introspection, display the model’s internal states:

  * **Latent Map:** Decide how to visualize the complex latent of shape \[C,H,W]. One approach: take the magnitude of one channel or a combination. For example, select a representative channel (maybe the one corresponding to some feature) and show its values as a heatmap. Compute `lat_vis = |L[channel_k]|` or `L.real[channel]` and normalize to 0-255. Use OpenCV’s `cv2.imshow` or ImGui image to show this 480×480 heatmap. Optionally, allow the user to pick which channel to view via a slider (1 to C). If C isn’t huge, you could tile multiple channels in a grid on one window.
  * **Wavelet Visualizer:** If wavelet transform is used, show the coefficients. For instance, after computing wavelet on the input or latent, take the sub-band images (LL, LH, HL, HH at level 1) and rescale each to visible range. You can arrange them in a mosaic: LL in top-left, LH top-right, HL bottom-left, HH bottom-right (commonly done for wavelet viz). Display that mosaic each frame to see how the model represents info at different scales. Update it when latent updates. This can help debug if the model is focusing on high-freq or low-freq.
  * **Memory Contents:** Visualize aspects of the associative memory. If storing small patches, you could display them as tiny images. For example, if a pattern corresponds to a latent patch 16×16, render it (maybe just one channel or heatmap of magnitude). If patterns are abstract vectors, you can at least show a matrix of memory weights (as an image where brightness = magnitude or phase). Another approach: show a similarity matrix of memory patterns (K×K matrix where entry (i,j) = |〈p\_i, p\_j〉|) as an image – if memory is orthonormal, this should be near identity. Compute that and display as a small grid image.
  * Provide UI controls to select what to view (e.g. radio buttons for “View latent channel vs memory vs wavelet”). Use appropriate windows for each.

* [ ] **Manual Weight Perturbation:** Add UI elements to poke the network’s weights:

  * For example, a button "Add Noise to Weights" which when clicked adds a small Gaussian noise (e.g. 0.01 std) to all trainable parameters (conv filters, A matrix, etc.). Implement by iterating through model parameters and adding noise in-place (within a `torch.no_grad()` block).
  * A button "Reset A to 0 (no Koopman)" or similar, which sets the generator `A` to zero matrix (so U becomes identity). This can test how important the Koopman dynamics are.
  * Sliders to zero out certain channels or set a global multiplier on certain weights (like multiply all conv weights by factor from 0 to 1).
  * Ensure when these actions are triggered, you lock the model from updating concurrently (maybe pause training momentarily or ensure thread safety if multithreaded). Since our loop is likely single-threaded, doing it in the same loop when button pressed is fine.
  * These perturbations help in understanding model robustness; they aren’t part of normal operation but are useful for interactive experimentation.

* [ ] **Module Freezing Toggles:** Provide checkboxes for each major module (NCA, Wavelet, Memory, Predictor, Action policy, etc.) to enable/disable learning or updates:

  * “Freeze NCA weights”: if checked, skip updating NCA conv filters and `A` in the optimizer (you can achieve this by setting their `requires_grad=False` or by removing them from optimizer temporarily).
  * “Freeze Memory writing”: if checked, do not perform the Hebbian memory update on new patterns (the memory recall can still happen). Implement via a simple `if not freeze_memory: memory.write(p)` logic.
  * “Freeze Predictor”: similar to NCA freeze.
  * “Freeze DSL execution”: maybe a toggle to turn off DSL influence (the agent then relies purely on learned policy).
  * Each toggle can correspond to a boolean that the update loop and training loop respect.
  * This allows analyzing behavior with certain learning aspects turned off, and also can stabilize things if needed (e.g. freeze some part that’s converged and let others catch up).

* [ ] **Save/Load Checkpoint Buttons:** Implement buttons to save the model state to disk and load it back:

  * “Save Model”: on click, serialize all necessary components – e.g. use `torch.save(model.state_dict(), "model.pth")` for the PyTorch model (including NCA, predictor, encoder, policy, etc.). Also save any stateful things like the memory patterns (since those are not in state\_dict if you just store them in a Python list) and the DSL program (if it’s dynamically generated). You could create a dict with {'model': state\_dict, 'memory': memory\_list, 'program': current\_program} and torch.save that.
  * “Load Model”: on click, do the inverse: `state = torch.load("model.pth")`, then `model.load_state_dict(state['model'])`, restore memory and program from the dict. After loading, perhaps set flags to reinitialize optimizer if needed (learning rate might remain same).
  * Ensure compatibility: if you save and load in the middle of an interactive session, verify that things continue smoothly (e.g. the loaded memory is properly normalized, etc.). It’s mainly a user convenience to test training persistence.

* [ ] **DSL Rerun/Editing:** If a DSL program is part of the agent’s operation, provide an interface to edit or select DSL scripts:

  * Perhaps have a text input box (if using ImGui) where one can type a DSL expression or program. This might be cumbersome in real-time, but at least allow loading one from a file or selecting from presets.
  * Provide a dropdown of predefined programs (for example: one program that makes the robot stack blocks, another that causes it to wave an arm). Selecting one sets the current DSL program in the agent.
  * A “Rerun Program” button: this would reset the environment and restart execution of the current DSL program from scratch. For instance, if you just edited or loaded a program for block stacking, you hit rerun to reset block positions and let the program go again. Implement environment reset (use stored initial positions, re-load URDFs if needed, or reposition them).
  * While the program runs, you might visualize the program pointer or current step in UI. If using ImGui, you could highlight the current AST node or print the current line being executed (some debug info).
  * This ability to rerun is useful for debugging DSL logic without interference from prior learning. Ensure that when a program is running, the learning loop can optionally be paused (you might want to observe pure program execution). Possibly tie the “DSL rerun” to also freezing neural updates momentarily, unless the program is meant to adapt as it runs.

* [ ] **UI Refresh and Event Handling:** Integrate the UI update into the main loop. If using ImGui, call `imgui.new_frame()` at loop start, build your UI elements (sliders, etc.), then render it with `imgui.render()`. If OpenCV windows, update trackbar positions from variables and use `cv2.imshow` to show images. Use non-blocking wait (e.g. `cv2.waitKey(1)` each loop) to handle window events. For ImGui with OpenGL or SDL, ensure you call the appropriate swap buffers.

  * Maintain a consistent frame rate for UI if possible (30-60 FPS). If the physics is running faster, you might not need to render every physics step.
  * If using OpenCV, note that `cv2.imshow` and `waitKey` drive the UI loop; ensure `waitKey` is called or windows won't update. Also handle window close events (if user manually closes the window, maybe break the loop gracefully).

* [ ] **Testing UI Controls:** Manually verify each UI element:

  * Move the speed slider and confirm simulation accelerates or slows (e.g. ball bounces faster).
  * Pause and resume, step one frame at a time to see if model and physics step correspondingly.
  * Toggle freeze on modules: e.g. freeze predictor and see if loss stops updating for that part, freeze memory to see if new patterns stop being added, etc.
  * Press weight perturbation and observe if anything changes (e.g. latent visualization might flicker due to noise injection).
  * Save model, change something (e.g. distort weights), then load model and see if it returns to previous behavior.
  * Edit or select a DSL program (if possible) and rerun to see intended effect (like the robot executes the scripted sequence).
  * Check that UI doesn’t lag the simulation significantly. If ImGui is too slow to render huge images, you might need to scale down the display or update less frequently.

## 9. Global Integration Requirements

* [ ] **End-to-End Autograd Graph:** Verify that all computational components (except the physics engine and explicit DSL logic for control) are connected via PyTorch autograd so that gradients propagate properly. Key places to check:

  * The NCA’s convolution and Cayley transform on `A` should produce gradients for their parameters (conv filters and `A` via its param rep) when the loss comes from the predictor.
  * The wavelet transform modules `DWTForward` and `DWTInverse` should be autograd-enabled (they are, using PyTorch under the hood). Test by feeding a tensor through DWT forward+inverse and checking `.requires_grad` flows.
  * The predictor and projection networks obviously are standard autograd.
  * The action mapping network (policy) receives gradients if the loss involves actions (e.g. future state error that action could influence in an intrinsic way). Initially, we might not have an explicit supervised loss on actions (if no external reward yet), but if implementing intrinsic objectives (like forward model loss), that indirectly shapes the policy.
  * If you incorporate a value or reward prediction network, ensure it’s also in the graph.
  * The DSL module: if any part of it has continuous parameters (say you decide to differentiate through selected subroutines or if you treat the DSL program selection as a soft embedding), ensure those are connected. However, for now DSL is mostly discrete logic, so you might not have autograd through it.

* [ ] **Complex Number Support:** Use PyTorch complex tensor operations wherever possible for clarity and performance. For example, represent `A` as a complex tensor of shape \[C,C] (`dtype=torch.cfloat`) so that `(I+A)` inversion and multiplication yield complex results directly. The convolution part might be easiest to handle as two real convolutions, but after that, combine into a complex tensor via `torch.view_as_complex(torch.stack([real_out, imag_out], dim=-1))`. Also use `tensor.real` and `tensor.imag` properties when you need to split. Make sure to register any custom complex ops with autograd if needed (most PyTorch ops support complex out of the box, including matmul, inverse, FFT, etc.). Check that no unsupported ops are used: e.g. `torch.nn.Conv2d` does not accept complex dtype as of now, hence the workaround of splitting real/imag. All other linear algebra for unitary should work in complex dtype. Test a forward and backward pass of a dummy complex operation to catch issues early (for instance, if accidentally using an operation that discards the imaginary part).

* [ ] **Dependency Management:** Limit external dependencies strictly to the allowed ones:

  * PyTorch (including torchvision if needed for any utility, though likely not needed).
  * PyBullet.
  * ImGui (via pyimgui or imgui-datascience which itself depends on pyimgui).
  * `pytorch_wavelets`.
  * Do **not** use other libraries like NumPy for core computations unless necessary. If you use NumPy (e.g. to prepare an image from pybullet), be careful to convert to torch tensor for model. Don’t use NumPy in any part that requires gradient.
  * Avoid other ML libs or heavy GUI libs. OpenCV (cv2) is fine since it’s used for display (and maybe image conversion). If using OpenCV, ensure it’s installed and note that it’s not strictly in the list but implied by mention of cv2. If not, rely purely on pyimgui for UI.
  * The script should be self-contained in one file: so avoid creating multiple modules. You can define multiple classes (for NCA, Memory, etc.) in the same file. If using pybullet\_data for URDFs, that’s fine (it’s part of pybullet). Document at top of script the needed packages so the user can install them.

* [ ] **Real-Time Performance on GPU:** Optimize for running on, say, an NVIDIA RTX 3090 (which has ample compute, but we should still be efficient):

  * Move all heavy tensor ops to GPU (`device = torch.device('cuda')`). Do `model.to(device)` and also ensure to send input images to device. Keep wavelet ops on GPU (they will if input is on GPU).
  * Minimize device transfers: e.g., when rendering images via OpenCV, you’ll have to bring a copy to CPU to display. That’s fine, but avoid copying large tensors too frequently. Perhaps do latent visualization by a small reduction on GPU then copy a small image to CPU.
  * If certain parts of the model don’t need high precision, consider using half precision (`model.half()`) – though mixing that with complex numbers might be problematic since PyTorch complex doesn’t support half as of writing (likely use float32 for complex). Probably keep float32 for now.
  * Profile the loop: the likely bottlenecks are PyBullet (if many objects or high freq) and the image rendering. If frame rate is low, consider lowering resolution or update rate of certain visualizations. The computational heavy neural parts (NCA conv on 480×480 with, say, C=16 channels is not too bad on a 3090, and predictor MLP is trivial).
  * If the system is still not real-time, one strategy is to drop physics FPS or skip neural updates some frames. But target real-time by balancing load rather than adding hacks.

* [ ] **Testing on RTX 3090:** When you can, run the integrated system on a machine with an RTX 3090 (or comparable). Ensure that the GPU is utilized (monitor usage with nvidia-smi). Check that the CPU usage (for PyBullet and UI) doesn’t bottleneck things. Ideally, you get \~30 FPS or more with moderate complexity. If not, identify the slowest part:

  * If PyBullet is slow, reduce substeps or complexity.
  * If rendering is slow, reduce resolution or frequency of `getCameraImage`.
  * If the model forward/backward is slow, consider reducing channel count or simplifying the network (since it’s single-file, you might have some margin to tune).
  * Memory and DSL logic are lightweight in comparison, unless you wrote a very inefficient interpreter (if it’s Python recursion heavy, that could slow things if program runs many steps per frame).

* [ ] **Final Checks and Logging:** Print or log key information for reassurance:

  * At startup, print devices in use (e.g. “Using GPU:0 Tesla RTX 3090”).
  * When switching environments or resetting, maybe print a message (“Resetting block stacking scenario”).
  * If any module triggers an error or constraint violation large, log that (or assert to catch bugs).
  * Possibly maintain a live display of some metrics in the UI: e.g. current InfoNCE loss, magnitude of unitary constraint $||A^H + A||$, etc. This can be done with text or plot if ambitious (ImGui can plot graphs). This helps user see that things are working (e.g. loss decreasing).

  Ensure no fatal errors on runtime. Handle exceptions gracefully: e.g. if file not found on load, catch it and display a UI message. Since it’s interactive, better avoid crashing out; instead, print error to console or UI and continue (maybe pause simulation if something goes wrong).

By following this checklist, we will implement a **single-file, GPU-efficient PyTorch system** integrating the Koopman NCA, multi-scale wavelet modeling, complex Hopfield memory, JEPA contrastive learning, an active-inference style training loop with constraints, a symbolic program module, physics simulation, and a rich UI for real-time interaction – all components communicating through shared latent tensors and adhering to differentiable design where applicable. Each item above should be checked off during development to ensure completeness and coherence of the final system.
